{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Google Colab Connection**"
      ],
      "metadata": {
        "id": "-LV43TytXzAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp6oz3wPXlnQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unzipping Dataset Content**"
      ],
      "metadata": {
        "id": "tXi8esmZX5vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/CKplus.zip\" -d \"/content/dataset\""
      ],
      "metadata": {
        "id": "fcnRQvP2YA5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basline Model Training (Change data paths for each dataset**)"
      ],
      "metadata": {
        "id": "RUxdp4G4YGca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multitask Emotion & Gender Classification with CBAM, Fairness, and wandb\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from itertools import cycle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import scipy.ndimage as snd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"multitask-gender-emotion\", name=\"PH-CBAM-CNN-CKplus\")\n",
        "\n",
        "#  Configuration\n",
        "emotion_labels = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
        "gender_labels = [\"Man\", \"Woman\"]\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "LR = 1e-4\n",
        "\n",
        "CSV_PATH      = \"/content/dataset/CK+/processed/CKPLUS_Features_20250805_111308_faceprefixed_CLEAN.csv\"\n",
        "TRAIN_IMG_DIR = \"/content/dataset/CK+/processed/train\"\n",
        "VAL_IMG_DIR   = \"/content/dataset/CK+/processed/val\"\n",
        "TEST_IMG_DIR  = \"/content/dataset/CK+/processed/test\"\n",
        "\n",
        "\n",
        "MODEL_PATH = \"multitask_model_ph_cbam.pt\"\n",
        "PRED_CSV   = \"multitask_predictions_ph_cbam.csv\"\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "emo_le = LabelEncoder().fit(emotion_labels)\n",
        "gen_le = LabelEncoder().fit(gender_labels)\n",
        "\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "\n",
        "    # Better center-preserving zoom (avoids cutting off facial regions)\n",
        "    transforms.RandomResizedCrop(224, scale=(0.85, 1.0), ratio=(0.95, 1.05)),\n",
        "\n",
        "    transforms.RandomHorizontalFlip(p=0.5),      # Standard flip\n",
        "    transforms.RandomRotation(degrees=10),       # Conservative for CK+ expressions\n",
        "\n",
        "    # Mild jitter to simulate different lighting environments\n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.15,\n",
        "        contrast=0.15,\n",
        "        saturation=0.15,\n",
        "        hue=0.02\n",
        "    ),\n",
        "\n",
        "    # Rare but realistic grayscale conversion (retains fine texture)\n",
        "    transforms.RandomGrayscale(p=0.03),\n",
        "\n",
        "    # Light blur to help generalize to lower-res datasets\n",
        "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 0.8)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Very light erasing (important: aggressive values ruin expression detail)\n",
        "    transforms.RandomErasing(\n",
        "        p=0.05,\n",
        "        scale=(0.01, 0.05),\n",
        "        ratio=(0.5, 2.0),\n",
        "        value='random'\n",
        "    ),\n",
        "\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "#  PH-CBAM Attention Module\n",
        "class PH_CBAM(nn.Module):\n",
        "    def __init__(self, in_channels, ratio=8):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.shared_mlp = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_channels, in_channels // ratio),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_channels // ratio, in_channels)\n",
        "        )\n",
        "        self.spatial = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Parallel Channel Attention\n",
        "        avg_out = self.shared_mlp(self.avg_pool(x)).unsqueeze(-1).unsqueeze(-1)\n",
        "        max_out = self.shared_mlp(self.max_pool(x)).unsqueeze(-1).unsqueeze(-1)\n",
        "        ch_att = self.sigmoid(avg_out + max_out)\n",
        "        x_ch = x * ch_att\n",
        "\n",
        "        # Parallel Spatial Attention\n",
        "        max_out, _ = torch.max(x_ch, dim=1, keepdim=True)\n",
        "        avg_out = torch.mean(x_ch, dim=1, keepdim=True)\n",
        "        sa_input = torch.cat([avg_out, max_out], dim=1)\n",
        "        sa_att = self.spatial(sa_input)\n",
        "        return x_ch * sa_att\n",
        "\n",
        "class AffectNetDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None, verbose=True):\n",
        "        \"\"\"\n",
        "        df         : DataFrame slice for the split\n",
        "        img_dir    : path to images for that split\n",
        "        transform  : torchvision transform pipeline (or None)\n",
        "        verbose    : whether to print every 1000th image load\n",
        "        \"\"\"\n",
        "        self.df        = df.reset_index(drop=True)\n",
        "        self.img_dir   = img_dir\n",
        "        self.transform = transform\n",
        "        self.verbose   = verbose\n",
        "        print(f\"CK_plus â†’ {len(self.df)} samples from {self.img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row[\"filename\"])\n",
        "\n",
        "        if self.verbose and idx % 1000 == 0:\n",
        "            print(f\"[INFO] Loading image {idx}/{len(self.df)}: {img_path}\")\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            # pass raw ndarray, transform will convert to PIL\n",
        "            img_tensor = self.transform(img)\n",
        "        else:\n",
        "            img = img.astype(np.float32)\n",
        "            img = (img - img.mean()) / (img.std() + 1e-5)\n",
        "            img_tensor = torch.tensor(img).permute(2, 0, 1).float()\n",
        "\n",
        "        emo = torch.tensor(emo_le.transform([row[\"dominant_emotion\"]])[0])\n",
        "        gen = torch.tensor(gen_le.transform([row[\"gender\"]])[0])\n",
        "\n",
        "        return img_tensor, emo, gen\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  Multi-Task CNN Model with PH-CBAM\n",
        "class MultiTaskCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(64), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(128), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(256), nn.MaxPool2d(2),\n",
        "            PH_CBAM(256)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256 * 28 * 28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.head_emo = nn.Linear(512, len(emotion_labels))\n",
        "        self.head_gen = nn.Linear(512, len(gender_labels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return self.head_emo(x), self.head_gen(x)\n",
        "\n",
        "#  Dataloader Factory\n",
        "def make_loader(df, img_dir, shuffle=True, train=False):\n",
        "    tfm = train_transform if train else eval_transform\n",
        "    return DataLoader(\n",
        "        AffectNetDataset(df, img_dir, transform=tfm),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "\n",
        "#  Basic Evaluation Function\n",
        "def evaluate(model, loader, split_name=\"\"):\n",
        "    model.eval()\n",
        "    true_emo, pred_emo, true_gen, pred_gen = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, emo_y, gen_y) in enumerate(tqdm(loader, desc=f\"Evaluating {split_name}\")):\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            emo_y = emo_y.to(DEVICE)\n",
        "            gen_y = gen_y.to(DEVICE)\n",
        "\n",
        "            emo_logits, gen_logits = model(imgs)\n",
        "            true_emo.extend(emo_y.cpu().numpy())\n",
        "            pred_emo.extend(torch.argmax(emo_logits, dim=1).cpu().numpy())\n",
        "            true_gen.extend(gen_y.cpu().numpy())\n",
        "            pred_gen.extend(torch.argmax(gen_logits, dim=1).cpu().numpy())\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"true_emotion\": emo_le.inverse_transform(np.array(true_emo)),\n",
        "        \"pred_emotion\": emo_le.inverse_transform(np.array(pred_emo)),\n",
        "        \"true_gender\": gen_le.inverse_transform(np.array(true_gen)),\n",
        "        \"pred_gender\": gen_le.inverse_transform(np.array(pred_gen)),\n",
        "    })\n",
        "    return df\n",
        "\n",
        "# Confusion Matrix Helper (with percentages)\n",
        "def plot_confusion(df, title=\"Confusion\"):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    #  Emotion Confusion Matrix\n",
        "    cm_emo = confusion_matrix(df[\"true_emotion\"], df[\"pred_emotion\"], labels=emotion_labels, normalize='true') * 100\n",
        "    sns.heatmap(\n",
        "        cm_emo, annot=True, fmt=\".1f\", cmap='Blues', ax=axes[0],\n",
        "        xticklabels=emotion_labels, yticklabels=emotion_labels,\n",
        "        cbar_kws={'label': 'Percentage (%)'}\n",
        "    )\n",
        "    axes[0].set_title(f\"{title} - Emotion\")\n",
        "    axes[0].set_xlabel(\"Predicted\")\n",
        "    axes[0].set_ylabel(\"True\")\n",
        "\n",
        "    #  Gender Confusion Matrix\n",
        "    cm_gen = confusion_matrix(df[\"true_gender\"], df[\"pred_gender\"], labels=gender_labels, normalize='true') * 100\n",
        "    sns.heatmap(\n",
        "        cm_gen, annot=True, fmt=\".1f\", cmap='Greens', ax=axes[1],\n",
        "        xticklabels=gender_labels, yticklabels=gender_labels,\n",
        "        cbar_kws={'label': 'Percentage (%)'}\n",
        "    )\n",
        "    axes[1].set_title(f\"{title} - Gender\")\n",
        "    axes[1].set_xlabel(\"Predicted\")\n",
        "    axes[1].set_ylabel(\"True\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({f\"{title}_conf_matrix\": wandb.Image(fig)})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# internal helper\n",
        "def _smooth(arr, win=2, do=True):\n",
        "    \"\"\"Uniform 1â€‘D smoothing for nicer curves.\"\"\"\n",
        "    if not do or len(arr) < win:\n",
        "        return np.asarray(arr)\n",
        "    return snd.uniform_filter1d(np.asarray(arr, dtype=float), size=win, mode=\"nearest\")\n",
        "\n",
        "#  Plot Accuracy Curves\n",
        "def plot_accuracy_curves(train_emo_accs, val_emo_accs,\n",
        "                         train_gen_accs,  val_gen_accs,\n",
        "                         smooth=True):\n",
        "    epochs = np.arange(1, len(train_emo_accs) + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_emo_accs, do=smooth), label=\"Train Emotion Acc\",  linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_emo_accs,  do=smooth), label=\"Val   Emotion Acc\",  linewidth=2)\n",
        "    plt.plot(epochs, _smooth(train_gen_accs, do=smooth), \"--\", label=\"Train Gender Acc\", linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_gen_accs,  do=smooth), \"--\", label=\"Val   Gender Acc\", linewidth=2)\n",
        "\n",
        "    # mark earlyâ€‘stop position\n",
        "    plt.axvline(x=len(epochs), color=\"grey\", linestyle=\":\", alpha=0.6)\n",
        "\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({\"accuracy_curves\": wandb.Image(plt)})\n",
        "\n",
        "# Plot Loss Curves\n",
        "def plot_loss_curves(train_losses, val_losses,\n",
        "                     train_emo,   val_emo,\n",
        "                     train_gen,   val_gen,\n",
        "                     smooth=True):\n",
        "    epochs = np.arange(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_losses, do=smooth), label=\"Train Total\", linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_losses,   do=smooth), label=\"Val   Total\", linewidth=2)\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_emo, do=smooth), \"--\", label=\"Train Emotion\")\n",
        "    plt.plot(epochs, _smooth(val_emo,   do=smooth), \"--\", label=\"Val   Emotion\")\n",
        "    plt.plot(epochs, _smooth(train_gen, do=smooth), \":\",  label=\"Train Gender\")\n",
        "    plt.plot(epochs, _smooth(val_gen,   do=smooth), \":\",  label=\"Val   Gender\")\n",
        "\n",
        "    plt.axvline(x=len(epochs), color=\"grey\", linestyle=\":\", alpha=0.6)\n",
        "\n",
        "    plt.title(\"Loss Curves\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({\"loss_curves\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "#  Collect predictions helper\n",
        "@torch.no_grad()\n",
        "def collect_preds(loader, return_probs: bool = False):\n",
        "    model.eval()\n",
        "    emo_true, emo_pred, gen_true, gen_pred = [], [], [], []\n",
        "    emo_probs, gen_probs = [], []\n",
        "\n",
        "    for imgs, e_y, g_y in loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        e_y = e_y.to(DEVICE)\n",
        "        g_y = g_y.to(DEVICE)\n",
        "\n",
        "        emo_logits, gen_logits = model(imgs)\n",
        "\n",
        "        emo_true.extend(e_y.cpu().numpy())\n",
        "        gen_true.extend(g_y.cpu().numpy())\n",
        "        emo_pred.extend(torch.argmax(emo_logits, 1).cpu().numpy())\n",
        "        gen_pred.extend(torch.argmax(gen_logits, 1).cpu().numpy())\n",
        "\n",
        "        if return_probs:\n",
        "            emo_probs.append(torch.softmax(emo_logits, 1).cpu())\n",
        "            gen_probs.append(torch.softmax(gen_logits, 1).cpu())\n",
        "\n",
        "    if return_probs:\n",
        "        return (\n",
        "            np.array(emo_true), np.array(emo_pred),\n",
        "            np.array(gen_true), np.array(gen_pred),\n",
        "            torch.cat(emo_probs).numpy(), torch.cat(gen_probs).numpy()\n",
        "        )\n",
        "    else:\n",
        "        return (\n",
        "            np.array(emo_true), np.array(emo_pred),\n",
        "            np.array(gen_true), np.array(gen_pred)\n",
        "        )\n",
        "\n",
        "# Extended ROC Curve Plot\n",
        "def plot_extended_roc(split_name, true_emo, prob_emo, true_gen, prob_gen):\n",
        "    true_emo_bin = label_binarize(true_emo, classes=list(range(len(emotion_labels))))\n",
        "    true_gen_bin = label_binarize(true_gen, classes=list(range(len(gender_labels))))\n",
        "\n",
        "    fpr_emo = dict()\n",
        "    tpr_emo = dict()\n",
        "    roc_auc_emo = dict()\n",
        "\n",
        "    for i in range(len(emotion_labels)):\n",
        "        fpr_emo[i], tpr_emo[i], _ = roc_curve(true_emo_bin[:, i], prob_emo[:, i])\n",
        "        roc_auc_emo[i] = auc(fpr_emo[i], tpr_emo[i])\n",
        "\n",
        "    fpr_emo[\"micro\"], tpr_emo[\"micro\"], _ = roc_curve(true_emo_bin.ravel(), prob_emo.ravel())\n",
        "    roc_auc_emo[\"micro\"] = auc(fpr_emo[\"micro\"], tpr_emo[\"micro\"])\n",
        "\n",
        "    all_fpr = np.unique(np.concatenate([fpr_emo[i] for i in range(len(emotion_labels))]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(len(emotion_labels)):\n",
        "        mean_tpr += np.interp(all_fpr, fpr_emo[i], tpr_emo[i])\n",
        "    mean_tpr /= len(emotion_labels)\n",
        "    fpr_emo[\"macro\"] = all_fpr\n",
        "    tpr_emo[\"macro\"] = mean_tpr\n",
        "    roc_auc_emo[\"macro\"] = auc(fpr_emo[\"macro\"], tpr_emo[\"macro\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(fpr_emo[\"micro\"], tpr_emo[\"micro\"], linestyle='--', label=f\"Emotion Micro-average (AUC = {roc_auc_emo['micro']:.2f})\")\n",
        "    plt.plot(fpr_emo[\"macro\"], tpr_emo[\"macro\"], linestyle='-',  label=f\"Emotion Macro-average (AUC = {roc_auc_emo['macro']:.2f})\")\n",
        "    colors = cycle(sns.color_palette(\"hls\", len(emotion_labels)))\n",
        "    for i, color in zip(range(len(emotion_labels)), colors):\n",
        "        plt.plot(fpr_emo[i], tpr_emo[i], color=color, label=f\"{emotion_labels[i]} (AUC = {roc_auc_emo[i]:.2f})\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title(f\"{split_name} Emotion ROC\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend(loc=\"lower right\", fontsize=\"small\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({f\"roc_curve/{split_name.lower()}_emotion\": wandb.Image(plt)})\n",
        "\n",
        "    fpr_gen = dict()\n",
        "    tpr_gen = dict()\n",
        "    roc_auc_gen = dict()\n",
        "\n",
        "    for i in range(len(gender_labels)):\n",
        "        fpr_gen[i], tpr_gen[i], _ = roc_curve(true_gen_bin[:, i], prob_gen[:, i])\n",
        "        roc_auc_gen[i] = auc(fpr_gen[i], tpr_gen[i])\n",
        "\n",
        "    fpr_gen[\"micro\"], tpr_gen[\"micro\"], _ = roc_curve(true_gen_bin.ravel(), prob_gen.ravel())\n",
        "    roc_auc_gen[\"micro\"] = auc(fpr_gen[\"micro\"], tpr_gen[\"micro\"])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_gen[\"micro\"], tpr_gen[\"micro\"], linestyle='--', label=f\"Gender Micro-average (AUC = {roc_auc_gen['micro']:.2f})\")\n",
        "    for i in range(len(gender_labels)):\n",
        "        plt.plot(fpr_gen[i], tpr_gen[i], label=f\"{gender_labels[i]} (AUC = {roc_auc_gen[i]:.2f})\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title(f\"{split_name} Gender ROC\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({f\"roc_curve/{split_name.lower()}_gender\": wandb.Image(plt)})\n",
        "\n",
        "#  Fairness Auditing: Gender Ã— Emotion Accuracy\n",
        "def fairness_audit(df):\n",
        "    print(\"\\n Accuracy by Gender Ã— Emotion:\")\n",
        "    accuracy_table = df.groupby([\"true_gender\", \"true_emotion\"]).apply(\n",
        "        lambda g: (g[\"true_emotion\"] == g[\"pred_emotion\"]).mean()\n",
        "    ).unstack().fillna(0)\n",
        "    print(accuracy_table.round(3))\n",
        "\n",
        "    for gender in accuracy_table.index:\n",
        "        for emotion in accuracy_table.columns:\n",
        "            wandb.log({f\"fairness/{gender}/{emotion}\": accuracy_table.loc[gender, emotion]})\n",
        "\n",
        "    pos_rate = df[\"pred_gender\"].value_counts(normalize=True)\n",
        "    disp_impact = pos_rate.get(\"Woman\", 0) / (pos_rate.get(\"Man\", 1e-6))\n",
        "    print(f\"\\n  Disparate Impact Ratio (Woman / Man): {disp_impact:.3f}\")\n",
        "    wandb.log({\"disparate_impact_ratio\": disp_impact})\n",
        "\n",
        "    woman_acc = accuracy_table.loc[\"Woman\"].mean()\n",
        "    man_acc = accuracy_table.loc[\"Man\"].mean()\n",
        "    equal_opp_diff = woman_acc - man_acc\n",
        "    print(f\"\\n Equal Opportunity Difference (Woman - Man): {equal_opp_diff:.3f}\")\n",
        "    wandb.log({\"equal_opportunity_diff\": equal_opp_diff})\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    accuracy_table.T.plot(kind=\"bar\")\n",
        "    plt.title(\"Emotion Classification Accuracy by Gender\")\n",
        "    plt.xlabel(\"Emotion\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y')\n",
        "    plt.legend(title=\"Gender\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({\"gender_bias_bar_chart\": wandb.Image(plt)})\n",
        "\n",
        "#  Main\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)\n",
        "    model = MultiTaskCNN().to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #Load CSV and Prepare DataLoaders\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    train_loader = make_loader(\n",
        "        df[df[\"dataset\"] == \"train\"],\n",
        "        TRAIN_IMG_DIR,\n",
        "        shuffle=True,\n",
        "        train=True  #  Apply data augmentation\n",
        "    )\n",
        "\n",
        "    val_loader = make_loader(\n",
        "        df[df[\"dataset\"] == \"val\"],\n",
        "        VAL_IMG_DIR,\n",
        "        shuffle=False,\n",
        "        train=False  # No augmentation\n",
        "    )\n",
        "\n",
        "    test_loader = make_loader(\n",
        "        df[df[\"dataset\"] == \"test\"],\n",
        "        TEST_IMG_DIR,\n",
        "        shuffle=False,\n",
        "        train=False  #  No augmentation\n",
        "    )\n",
        "\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "#  Optimizer and Scheduler\n",
        "initial_lr = 0.001  # Set your desired learning rate here\n",
        "opt = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
        "scheduler = StepLR(opt, step_size=5, gamma=0.5)  # Reduce LR by half every 5 epochs\n",
        "\n",
        "# === Tracking lists ===\n",
        "train_losses, val_losses = [], []\n",
        "train_emo_losses, train_gen_losses = [], []\n",
        "val_emo_losses, val_gen_losses = [], []\n",
        "\n",
        "train_emo_accuracies, val_emo_accuracies = [], []\n",
        "train_gen_accuracies, val_gen_accuracies = [], []\n",
        "\n",
        "#  Hybridâ€‘Earlyâ€‘Stopping Setup\n",
        "patience          = 5         # Max epochs to wait without improvement\n",
        "min_delta_loss    = 0.0005        # Minimum drop in loss to count as better\n",
        "min_delta_acc     = 0.001       # Minimum gain in accuracy to count as better\n",
        "\n",
        "best_val_loss     = float(\"inf\")\n",
        "best_val_acc      = 0.0\n",
        "best_epoch        = 0\n",
        "epochs_no_improve = 0\n",
        "#\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nðŸ“¦ Starting Epoch {epoch + 1}/{EPOCHS}\")\n",
        "\n",
        "    # -------- TRAIN --------\n",
        "    model.train()\n",
        "    total_loss = total_emo_loss = total_gen_loss = 0.0\n",
        "    correct_emo = total_emo = correct_gen = total_gen = 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"ðŸŒ€ Epoch {epoch+1}/{EPOCHS} [Training]\", leave=False)\n",
        "    for images, emo_labels, gen_labels in loop:\n",
        "        images, emo_labels, gen_labels = images.to(DEVICE), emo_labels.to(DEVICE), gen_labels.to(DEVICE)\n",
        "\n",
        "        emo_logits, gen_logits = model(images)\n",
        "        emo_loss = criterion(emo_logits, emo_labels)\n",
        "        gen_loss = criterion(gen_logits, gen_labels)\n",
        "        loss = emo_loss + gen_loss\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss      += loss.item()\n",
        "        total_emo_loss  += emo_loss.item()\n",
        "        total_gen_loss  += gen_loss.item()\n",
        "        correct_emo     += (torch.argmax(emo_logits, 1) == emo_labels).sum().item()\n",
        "        total_emo       += emo_labels.size(0)\n",
        "        correct_gen     += (torch.argmax(gen_logits, 1) == gen_labels).sum().item()\n",
        "        total_gen       += gen_labels.size(0)\n",
        "\n",
        "    avg_train_loss     = total_loss / len(train_loader)\n",
        "    avg_emo_loss       = total_emo_loss / len(train_loader)\n",
        "    avg_gen_loss       = total_gen_loss / len(train_loader)\n",
        "    train_emo_acc      = correct_emo / total_emo\n",
        "    train_gen_acc      = correct_gen / total_gen\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_emo_losses.append(avg_emo_loss)\n",
        "    train_gen_losses.append(avg_gen_loss)\n",
        "    train_emo_accuracies.append(train_emo_acc)\n",
        "    train_gen_accuracies.append(train_gen_acc)\n",
        "\n",
        "    #  VALIDATION\n",
        "    model.eval()\n",
        "    val_loss = val_emo_loss = val_gen_loss = 0.0\n",
        "    val_correct_emo = val_total_emo = val_correct_gen = val_total_gen = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, emo_labels, gen_labels in val_loader:\n",
        "            images, emo_labels, gen_labels = images.to(DEVICE), emo_labels.to(DEVICE), gen_labels.to(DEVICE)\n",
        "\n",
        "            emo_logits, gen_logits = model(images)\n",
        "            emo_l = criterion(emo_logits, emo_labels)\n",
        "            gen_l = criterion(gen_logits, gen_labels)\n",
        "\n",
        "            val_loss      += (emo_l + gen_l).item()\n",
        "            val_emo_loss  += emo_l.item()\n",
        "            val_gen_loss  += gen_l.item()\n",
        "            val_correct_emo += (torch.argmax(emo_logits, 1) == emo_labels).sum().item()\n",
        "            val_total_emo   += emo_labels.size(0)\n",
        "            val_correct_gen += (torch.argmax(gen_logits, 1) == gen_labels).sum().item()\n",
        "            val_total_gen   += gen_labels.size(0)\n",
        "\n",
        "    avg_val_loss      = val_loss / len(val_loader)\n",
        "    avg_val_emo_loss  = val_emo_loss / len(val_loader)\n",
        "    avg_val_gen_loss  = val_gen_loss / len(val_loader)\n",
        "    val_emo_acc       = val_correct_emo / val_total_emo\n",
        "    val_gen_acc       = val_correct_gen / val_total_gen\n",
        "\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_emo_losses.append(avg_val_emo_loss)\n",
        "    val_gen_losses.append(avg_val_gen_loss)\n",
        "    val_emo_accuracies.append(val_emo_acc)\n",
        "    val_gen_accuracies.append(val_gen_acc)\n",
        "\n",
        "    print(\n",
        "        f\" Epoch {epoch+1}: \"\n",
        "        f\"Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, \"\n",
        "        f\"Train Emo Acc={train_emo_acc:.4f}, Val Emo Acc={val_emo_acc:.4f}, \"\n",
        "        f\"Train Gen Acc={train_gen_acc:.4f}, Val Gen Acc={val_gen_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    # --- Log to Weights & Biases ---\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_train_loss,\n",
        "        \"val_loss\": avg_val_loss,\n",
        "        \"train_emo_loss\": avg_emo_loss,\n",
        "        \"val_emo_loss\": avg_val_emo_loss,\n",
        "        \"train_gen_loss\": avg_gen_loss,\n",
        "        \"val_gen_loss\": val_gen_loss,\n",
        "        \"train_emo_accuracy\": train_emo_acc,\n",
        "        \"val_emo_accuracy\": val_emo_acc,\n",
        "        \"train_gen_accuracy\": train_gen_acc,\n",
        "        \"val_gen_accuracy\": val_gen_acc,\n",
        "        \"lr\": opt.param_groups[0][\"lr\"],\n",
        "    })\n",
        "\n",
        "    # --- Step scheduler ---\n",
        "    scheduler.step()\n",
        "\n",
        "    # --- Hybrid Early Stopping ---\n",
        "    improved_loss = (best_val_loss - avg_val_loss) > min_delta_loss\n",
        "    improved_acc  = (val_emo_acc - best_val_acc) > min_delta_acc\n",
        "\n",
        "    if improved_loss or improved_acc:\n",
        "        epochs_no_improve = 0\n",
        "        best_epoch = epoch\n",
        "        if improved_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "        if improved_acc:\n",
        "            best_val_acc = val_emo_acc\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"ðŸ“¦ New best model saved at epoch {epoch+1} \"\n",
        "              f\"(val_loss={best_val_loss:.4f}, val_acc={best_val_acc:.4f})\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"  No improvement for {epochs_no_improve} epoch(s)\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\" Early stopping triggered at epoch {epoch+1} (best epoch was {best_epoch+1})\")\n",
        "\n",
        "        # â”€ Trim logs so curves end at best_epoch â”€\n",
        "        end = best_epoch + 1\n",
        "        train_losses         = train_losses[:end]\n",
        "        val_losses           = val_losses[:end]\n",
        "        train_emo_losses     = train_emo_losses[:end]\n",
        "        val_emo_losses       = val_emo_losses[:end]\n",
        "        train_gen_losses     = train_gen_losses[:end]\n",
        "        val_gen_losses       = val_gen_losses[:end]\n",
        "        train_emo_accuracies = train_emo_accuracies[:end]\n",
        "        val_emo_accuracies   = val_emo_accuracies[:end]\n",
        "        train_gen_accuracies = train_gen_accuracies[:end]\n",
        "        val_gen_accuracies   = val_gen_accuracies[:end]\n",
        "\n",
        "        break\n",
        "\n",
        "# END forâ€‘epoch LOOP\n",
        "\n",
        "# Final Evaluation\n",
        "print(\"\\n Training complete. Running final evaluation on best model...\")\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "train_df = evaluate(model, train_loader, split_name=\"train\")\n",
        "val_df   = evaluate(model, val_loader,   split_name=\"val\")\n",
        "test_df  = evaluate(model, test_loader,  split_name=\"test\")\n",
        "print(\" Evaluation complete.\")\n",
        "\n",
        "# Save test predictions\n",
        "test_df.to_csv(PRED_CSV, index=False)\n",
        "print(f\" Saved test predictions to {PRED_CSV}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion(train_df, title=\"Train Confusion Matrix\")\n",
        "plot_confusion(val_df,   title=\"Validation Confusion Matrix\")\n",
        "plot_confusion(test_df,  title=\"Test Confusion Matrix\")\n",
        "\n",
        "#\n",
        "#  Classification reports, curves, ROC plots\n",
        "#\n",
        "def print_classification_reports(df, split_name=\"\"):\n",
        "    print(f\"\\n {split_name} Emotion Classification Report:\")\n",
        "    print(classification_report(df[\"true_emotion\"],\n",
        "                                df[\"pred_emotion\"],\n",
        "                                target_names=emotion_labels))\n",
        "\n",
        "    print(f\"\\n {split_name} Gender Classification Report:\")\n",
        "    print(classification_report(df[\"true_gender\"],\n",
        "                                df[\"pred_gender\"],\n",
        "                                target_names=gender_labels))\n",
        "\n",
        "print_classification_reports(train_df, \"Train\")\n",
        "print_classification_reports(val_df,   \"Validation\")\n",
        "print_classification_reports(test_df,  \"Test\")\n",
        "\n",
        "# Accuracy / loss curves\n",
        "plot_accuracy_curves(train_emo_accuracies, val_emo_accuracies,\n",
        "                     train_gen_accuracies,  val_gen_accuracies)\n",
        "\n",
        "plot_loss_curves(train_losses, val_losses,\n",
        "                 train_emo_losses, val_emo_losses,\n",
        "                 train_gen_losses, val_gen_losses)\n",
        "\n",
        "#\n",
        "#  Collect predictions (with probabilities) for ROC curves\n",
        "#\n",
        "(\n",
        "    emo_true_train, _, gen_true_train, _, emo_prob_train, gen_prob_train\n",
        ") = collect_preds(train_loader, return_probs=True)\n",
        "(\n",
        "    emo_true_val,   _, gen_true_val,   _, emo_prob_val,   gen_prob_val\n",
        ") = collect_preds(val_loader,   return_probs=True)\n",
        "(\n",
        "    emo_true_test,  _, gen_true_test,  _, emo_prob_test,  gen_prob_test\n",
        ") = collect_preds(test_loader,  return_probs=True)\n",
        "\n",
        "#  ensure gender prob arrays have two columns (binary case)\n",
        "def _ensure_two_cols(arr):\n",
        "    \"\"\"If arr is shape [N] or [N,1], convert to [N,2] as [1-p, p].\"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1 or (arr.ndim == 2 and arr.shape[1] == 1):\n",
        "        arr = arr.reshape(-1, 1)\n",
        "        arr = np.hstack([(1.0 - arr), arr])\n",
        "    return arr\n",
        "\n",
        "gen_prob_train = _ensure_two_cols(gen_prob_train)\n",
        "gen_prob_val   = _ensure_two_cols(gen_prob_val)\n",
        "gen_prob_test  = _ensure_two_cols(gen_prob_test)\n",
        "\n",
        "#\n",
        "#  Plot ROC curves\n",
        "#\n",
        "plot_extended_roc(\"Train\",      emo_true_train, emo_prob_train,\n",
        "                  gen_true_train, gen_prob_train)\n",
        "plot_extended_roc(\"Validation\", emo_true_val,   emo_prob_val,\n",
        "                  gen_true_val,   gen_prob_val)\n",
        "plot_extended_roc(\"Test\",       emo_true_test,  emo_prob_test,\n",
        "                  gen_true_test,  gen_prob_test)\n",
        "\n",
        "#\n",
        "#  Fairness Auditing: Gender Ã— Emotion Analysis\n",
        "#\n",
        "def fairness_audit(df):\n",
        "    print(\"\\n Accuracy by Gender Ã— Emotion:\")\n",
        "    accuracy_table = (\n",
        "        df.groupby([\"true_gender\", \"true_emotion\"])\n",
        "          .apply(lambda g: (g[\"true_emotion\"] == g[\"pred_emotion\"]).mean())\n",
        "          .unstack()\n",
        "          .fillna(0)\n",
        "    )\n",
        "    print(accuracy_table.round(3))\n",
        "\n",
        "    # Log each cell to wandb\n",
        "    for gender in accuracy_table.index:\n",
        "        for emotion in accuracy_table.columns:\n",
        "            wandb.log({f\"fairness/{gender}/{emotion}\": accuracy_table.loc[gender, emotion]})\n",
        "\n",
        "    #  Disparate Impact\n",
        "    pos_rate    = df[\"pred_gender\"].value_counts(normalize=True)\n",
        "    disp_impact = pos_rate.get(\"Woman\", 0) / (pos_rate.get(\"Man\", 1e-6))\n",
        "    print(f\"\\n  Disparate Impact Ratio (Woman / Man): {disp_impact:.3f}\")\n",
        "    wandb.log({\"disparate_impact_ratio\": disp_impact})\n",
        "\n",
        "    # Equal Opportunity\n",
        "    woman_acc      = accuracy_table.loc[\"Woman\"].mean()\n",
        "    man_acc        = accuracy_table.loc[\"Man\"].mean()\n",
        "    equal_opp_diff = woman_acc - man_acc\n",
        "    print(f\"\\n Equal Opportunity Difference (Woman - Man): {equal_opp_diff:.3f}\")\n",
        "    wandb.log({\"equal_opportunity_diff\": equal_opp_diff})\n",
        "\n",
        "    #  Bias Visualization Bar Chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    accuracy_table.T.plot(kind=\"bar\")\n",
        "    plt.title(\"Emotion Classification Accuracy by Gender\")\n",
        "    plt.xlabel(\"Emotion\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y')\n",
        "    plt.legend(title=\"Gender\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({\"gender_bias_bar_chart\": wandb.Image(plt)})\n",
        "\n",
        "#\n",
        "#  Run the fairness audit on the test set\n",
        "#\n",
        "fairness_audit(test_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "xD4l1ngRYelu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MobileNet_V3 Model Training (Change data paths for each dataset**)"
      ],
      "metadata": {
        "id": "mxFZL8EvZiGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Multitask Emotion & Gender Classification with MobileNetâ€‘V3, Fairness, and wandb\n",
        "# (CBAM & customâ€‘CNN removed â€“ MobileNetâ€¯V3 Small IMAGENETâ€‘preâ€‘trained backbone used)\n",
        "\n",
        "import os, cv2, io, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from itertools import cycle\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             roc_curve, auc)\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "import scipy.ndimage as snd\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torchvision.models import MobileNet_V3_Small_Weights as MNV3W\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import wandb\n",
        "wandb.init(project=\"multitask-gender-emotion\", name=\"CKplus-Scratchâ€‘MobileNetV3\")\n",
        "\n",
        "# Config\n",
        "emotion_labels  = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
        "gender_labels   = [\"Man\", \"Woman\"]\n",
        "\n",
        "IMG_SIZE  = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS   = 50\n",
        "LR = 1e-3\n",
        "\n",
        "CSV_PATH      = \"/content/dataset/CK+/processed/CKPLUS_Features_20250805_111308_faceprefixed_CLEAN.csv\"\n",
        "TRAIN_IMG_DIR = \"/content/dataset/CK+/processed/train\"\n",
        "VAL_IMG_DIR   = \"/content/dataset/CK+/processed/val\"\n",
        "TEST_IMG_DIR  = \"/content/dataset/CK+/processed/test\"\n",
        "\n",
        "\n",
        "MODEL_PATH = \"multitask_Scratch_model_mnv3.pt\"\n",
        "PRED_CSV   = \"multitask_Scratch_predictions_mnv3.csv\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "emo_le = LabelEncoder().fit(emotion_labels)\n",
        "gen_le = LabelEncoder().fit(gender_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "#  Optimized Train Transform for CK+ Dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.90, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomRotation(8),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "#  Evaluation Transform (No Augmentation)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Your Dataset Class (unchanged)\n",
        "class AffectNetDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None, verbose=True):\n",
        "        self.df, self.img_dir, self.transform, self.verbose = df.reset_index(drop=True), img_dir, transform, verbose\n",
        "        print(f\"CKplus â†’ {len(self.df)} samples from {self.img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        path = os.path.join(self.img_dir, row[\"filename\"])\n",
        "\n",
        "        if self.verbose and idx % 1000 == 0:\n",
        "            print(f\"[INFO] Loading {idx}/{len(self.df)}: {path}\")\n",
        "\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            img = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        img = self.transform(img) if self.transform else transforms.ToTensor()(img / 255.)\n",
        "\n",
        "        emo = torch.tensor(emo_le.transform([row[\"dominant_emotion\"]])[0])\n",
        "        gen = torch.tensor(gen_le.transform([row[\"gender\"]])[0])\n",
        "        return img, emo, gen\n",
        "\n",
        "#  Your make_loader Function\n",
        "def make_loader(df, img_dir, *, shuffle, train):\n",
        "    tfm = train_transform if train else eval_transform\n",
        "    return DataLoader(\n",
        "        AffectNetDataset(df, img_dir, transform=tfm),\n",
        "        batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=0\n",
        "    )\n",
        "\n",
        "\n",
        "#  New: Create all loaders\n",
        "def create_dataloaders(csv_path=CSV_PATH,\n",
        "                       train_img_dir=TRAIN_IMG_DIR,\n",
        "                       val_img_dir=VAL_IMG_DIR,\n",
        "                       test_img_dir=TEST_IMG_DIR,\n",
        "                       batch_size=BATCH_SIZE):\n",
        "    # Load full CSV\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Split df by your 'split' column (make sure your CSV has this)\n",
        "    train_df = df[df['dataset'] == 'train'].reset_index(drop=True)\n",
        "    val_df = df[df['dataset'] == 'val'].reset_index(drop=True)\n",
        "    test_df = df[df['dataset'] == 'test'].reset_index(drop=True)\n",
        "\n",
        "    # Create loaders using your make_loader func and existing transforms\n",
        "    train_loader = make_loader(train_df, train_img_dir, shuffle=True, train=True)\n",
        "    val_loader = make_loader(val_df, val_img_dir, shuffle=False, train=False)\n",
        "    test_loader = make_loader(test_df, test_img_dir, shuffle=False, train=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def mixup_data(x, y1, y2, alpha=0.4):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y1_a, y1_b = y1, y1[index]\n",
        "    y2_a, y2_b = y2, y2[index]\n",
        "    return mixed_x, y1_a, y1_b, y2_a, y2_b, lam\n",
        "\n",
        "\n",
        "#  usage\n",
        "train_loader, val_loader, test_loader = create_dataloaders()\n",
        "\n",
        "\n",
        "#  Multiâ€‘Task MobileNetâ€‘V3\n",
        "class MultiTaskCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        backbone = models.mobilenet_v3_small(weights=None,\n",
        "                                              norm_layer=lambda nf: nn.GroupNorm(8, nf))\n",
        "        self.features = backbone.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout1 = nn.Dropout(0.4)   # added before pooling\n",
        "        self.dropout2 = nn.Dropout(0.5)   # added before FC\n",
        "        self.head_emo = nn.Linear(576, len(emotion_labels))\n",
        "        self.head_gen = nn.Linear(576, len(gender_labels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.dropout2(x)\n",
        "        return self.head_emo(x), self.head_gen(x)\n",
        "\n",
        "# Evaluation util\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, split_name=\"\"):\n",
        "    model.eval()\n",
        "    true_e, pred_e, true_g, pred_g = [], [], [], []\n",
        "    for imgs, emo_y, gen_y in tqdm(loader, desc=f\"Evaluating {split_name}\", leave=False):\n",
        "        imgs = imgs.to(DEVICE); emo_y = emo_y.to(DEVICE); gen_y = gen_y.to(DEVICE)\n",
        "        emo_logit, gen_logit = model(imgs)\n",
        "        true_e += emo_y.cpu().tolist()\n",
        "        pred_e += emo_logit.argmax(1).cpu().tolist()\n",
        "        true_g += gen_y.cpu().tolist()\n",
        "        pred_g += gen_logit.argmax(1).cpu().tolist()\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"true_emotion\": emo_le.inverse_transform(np.array(true_e)),\n",
        "        \"pred_emotion\": emo_le.inverse_transform(np.array(pred_e)),\n",
        "        \"true_gender\":  gen_le.inverse_transform(np.array(true_g)),\n",
        "        \"pred_gender\":  gen_le.inverse_transform(np.array(pred_g)),\n",
        "    })\n",
        "\n",
        "#  Confâ€‘Matrix helper\n",
        "def plot_confusion(df, title=\"Confusion\"):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    cm_e = confusion_matrix(df[\"true_emotion\"], df[\"pred_emotion\"],\n",
        "                            labels=emotion_labels, normalize='true')*100\n",
        "    sns.heatmap(cm_e, annot=True, fmt=\".1f\", cmap=\"Blues\",\n",
        "                xticklabels=emotion_labels, yticklabels=emotion_labels,\n",
        "                cbar_kws={'label':'%'}, ax=axes[0])\n",
        "    axes[0].set_title(f\"{title} â€“ Emotion\"); axes[0].set_xlabel(\"Pred\"); axes[0].set_ylabel(\"True\")\n",
        "\n",
        "    cm_g = confusion_matrix(df[\"true_gender\"], df[\"pred_gender\"],\n",
        "                            labels=gender_labels, normalize='true')*100\n",
        "    sns.heatmap(cm_g, annot=True, fmt=\".1f\", cmap=\"Greens\",\n",
        "                xticklabels=gender_labels, yticklabels=gender_labels,\n",
        "                cbar_kws={'label':'%'}, ax=axes[1])\n",
        "    axes[1].set_title(f\"{title} â€“ Gender\"); axes[1].set_xlabel(\"Pred\"); axes[1].set_ylabel(\"True\")\n",
        "\n",
        "    plt.tight_layout(); plt.show()\n",
        "    wandb.log({f\"{title}_conf_matrix\": wandb.Image(fig)})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "#  Classification reports, curves, ROC plots\n",
        "#\n",
        "def print_classification_reports(df, split_name=\"\"):\n",
        "    print(f\"\\n {split_name} Emotion Classification Report:\")\n",
        "    print(classification_report(df[\"true_emotion\"],\n",
        "                                df[\"pred_emotion\"],\n",
        "                                target_names=emotion_labels))\n",
        "\n",
        "    print(f\"\\n {split_name} Gender Classification Report:\")\n",
        "    print(classification_report(df[\"true_gender\"],\n",
        "                                df[\"pred_gender\"],\n",
        "                                target_names=gender_labels))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #  internal helper\n",
        "def _smooth(arr, win: int = 2, *, do: bool = True):\n",
        "    \"\"\"Uniform 1â€‘D smoothing for nicer curves.\"\"\"\n",
        "    if not do or len(arr) < win:\n",
        "        return np.asarray(arr)\n",
        "    return snd.uniform_filter1d(np.asarray(arr, dtype=float), size=win, mode=\"nearest\")\n",
        "\n",
        "\n",
        "# Plot Accuracy Curves\n",
        "def plot_accuracy_curves(train_emo_accs, val_emo_accs,\n",
        "                         train_gen_accs,  val_gen_accs,\n",
        "                         *, smooth: bool = True):\n",
        "    epochs = np.arange(1, len(train_emo_accs) + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_emo_accs, do=smooth), label=\"TrainÂ EmotionÂ Acc\",  linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_emo_accs,  do=smooth), label=\"ValÂ Â EmotionÂ Acc\",   linewidth=2)\n",
        "    plt.plot(epochs, _smooth(train_gen_accs, do=smooth), \"--\", label=\"TrainÂ GenderÂ Acc\", linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_gen_accs,  do=smooth), \"--\", label=\"ValÂ Â GenderÂ Acc\",  linewidth=2)\n",
        "\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.ylim(0, 1)\n",
        "    plt.grid(True, alpha=.3); plt.legend(); plt.tight_layout(); plt.show()\n",
        "    wandb.log({\"accuracy_curves\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "#  Plot Loss Curves\n",
        "def plot_loss_curves(train_losses, val_losses,\n",
        "                     train_emo,   val_emo,\n",
        "                     train_gen,   val_gen,\n",
        "                     *, smooth: bool = True):\n",
        "    epochs = np.arange(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_losses, do=smooth), label=\"TrainÂ Total\", linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_losses,   do=smooth), label=\"ValÂ Â Total\",  linewidth=2)\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_emo, do=smooth), \"--\", label=\"TrainÂ Emotion\")\n",
        "    plt.plot(epochs, _smooth(val_emo,   do=smooth), \"--\", label=\"ValÂ Â Emotion\")\n",
        "    plt.plot(epochs, _smooth(train_gen, do=smooth), \":\",  label=\"TrainÂ Gender\")\n",
        "    plt.plot(epochs, _smooth(val_gen,   do=smooth), \":\",  label=\"ValÂ Â Gender\")\n",
        "\n",
        "    plt.title(\"Loss Curves\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True, alpha=.3)\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "    wandb.log({\"loss_curves\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "#  Collect predictions helper (with optional probabilities)\n",
        "@torch.no_grad()\n",
        "def collect_preds(loader, return_probs: bool = False):\n",
        "    model.eval()\n",
        "    emo_true, emo_pred, gen_true, gen_pred = [], [], [], []\n",
        "    emo_probs, gen_probs = [], []\n",
        "\n",
        "    for imgs, e_y, g_y in loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        e_y  = e_y.to(DEVICE)\n",
        "        g_y  = g_y.to(DEVICE)\n",
        "\n",
        "        emo_logits, gen_logits = model(imgs)\n",
        "\n",
        "        emo_true.extend(e_y.cpu().numpy())\n",
        "        gen_true.extend(g_y.cpu().numpy())\n",
        "        emo_pred.extend(torch.argmax(emo_logits, 1).cpu().numpy())\n",
        "        gen_pred.extend(torch.argmax(gen_logits, 1).cpu().numpy())\n",
        "\n",
        "        if return_probs:\n",
        "            emo_probs.append(torch.softmax(emo_logits, 1).cpu())\n",
        "            gen_probs.append(torch.softmax(gen_logits, 1).cpu())\n",
        "\n",
        "    if return_probs:\n",
        "        return (\n",
        "            np.array(emo_true), np.array(emo_pred),\n",
        "            np.array(gen_true), np.array(gen_pred),\n",
        "            torch.cat(emo_probs).numpy(), torch.cat(gen_probs).numpy()\n",
        "        )\n",
        "    else:\n",
        "        return (\n",
        "            np.array(emo_true), np.array(emo_pred),\n",
        "            np.array(gen_true), np.array(gen_pred)\n",
        "        )\n",
        "\n",
        "# ensure gender prob arrays have two columns (binary case)\n",
        "def _ensure_two_cols(arr):\n",
        "    \"\"\"If arr is shape [N] or [N,1], convert to [N,2] as [1-p, p].\"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1 or (arr.ndim == 2 and arr.shape[1] == 1):\n",
        "        arr = arr.reshape(-1, 1)\n",
        "        # Assuming the single column is the probability for the second class (Woman)\n",
        "        # You might need to adjust this based on your LabelEncoder mapping\n",
        "        arr = np.hstack([(1.0 - arr), arr])\n",
        "    return arr\n",
        "\n",
        "\n",
        "#Extended ROC Curve Plot\n",
        "def plot_extended_roc(split_name, true_emo, prob_emo, true_gen, prob_gen):\n",
        "    true_emo_bin = label_binarize(true_emo, classes=list(range(len(emotion_labels))))\n",
        "    true_gen_bin = label_binarize(true_gen, classes=list(range(len(gender_labels))))\n",
        "\n",
        "    #  Emotion ROC\n",
        "    fpr_e, tpr_e, roc_auc_e = {}, {}, {}\n",
        "    for i in range(len(emotion_labels)):\n",
        "        fpr_e[i], tpr_e[i], _ = roc_curve(true_emo_bin[:, i], prob_emo[:, i])\n",
        "        roc_auc_e[i] = auc(fpr_e[i], tpr_e[i])\n",
        "\n",
        "    fpr_e[\"micro\"], tpr_e[\"micro\"], _ = roc_curve(true_emo_bin.ravel(), prob_emo.ravel())\n",
        "    roc_auc_e[\"micro\"] = auc(fpr_e[\"micro\"], tpr_e[\"micro\"])\n",
        "\n",
        "    all_fpr = np.unique(np.concatenate([fpr_e[i] for i in range(len(emotion_labels))]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(len(emotion_labels)):\n",
        "        mean_tpr += np.interp(all_fpr, fpr_e[i], tpr_e[i])\n",
        "    mean_tpr /= len(emotion_labels)\n",
        "    fpr_e[\"macro\"], tpr_e[\"macro\"] = all_fpr, mean_tpr\n",
        "    roc_auc_e[\"macro\"] = auc(fpr_e[\"macro\"], tpr_e[\"macro\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(fpr_e[\"micro\"], tpr_e[\"micro\"],\n",
        "             linestyle='--', label=f\"Microâ€‘avg (AUCÂ =Â {roc_auc_e['micro']:.2f})\")\n",
        "    plt.plot(fpr_e[\"macro\"], tpr_e[\"macro\"],\n",
        "             label=f\"Macroâ€‘avg (AUCÂ =Â {roc_auc_e['macro']:.2f})\")\n",
        "    colors = cycle(sns.color_palette(\"hls\", len(emotion_labels)))\n",
        "    for i, c in zip(range(len(emotion_labels)), colors):\n",
        "        plt.plot(fpr_e[i], tpr_e[i], color=c,\n",
        "                 label=f\"{emotion_labels[i]} (AUCÂ =Â {roc_auc_e[i]:.2f})\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--'); plt.title(f\"{split_name} Emotion ROC\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(fontsize=\"small\"); plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()\n",
        "    wandb.log({f\"roc/{split_name.lower()}_emotion\": wandb.Image(plt)})\n",
        "\n",
        "    #  Gender ROC\n",
        "    # Ensure gender probabilities have two columns\n",
        "    prob_gen = _ensure_two_cols(prob_gen)\n",
        "\n",
        "    fpr_g, tpr_g, roc_auc_g = {}, {}, {}\n",
        "    for i in range(len(gender_labels)):\n",
        "        fpr_g[i], tpr_g[i], _ = roc_curve(true_gen_bin[:, i], prob_gen[:, i])\n",
        "        roc_auc_g[i] = auc(fpr_g[i], tpr_g[i])\n",
        "    fpr_g[\"micro\"], tpr_g[\"micro\"], _ = roc_curve(true_gen_bin.ravel(), prob_gen.ravel())\n",
        "    roc_auc_g[\"micro\"] = auc(fpr_g[\"micro\"], tpr_g[\"micro\"])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_g[\"micro\"], tpr_g[\"micro\"],\n",
        "             linestyle='--', label=f\"Microâ€‘avg (AUCÂ =Â {roc_auc_g['micro']:.2f})\")\n",
        "    for i in range(len(gender_labels)):\n",
        "        plt.plot(fpr_g[i], tpr_g[i], label=f\"{gender_labels[i]} (AUCÂ =Â {roc_auc_g[i]:.2f})\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--'); plt.title(f\"{split_name} Gender ROC\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()\n",
        "    wandb.log({f\"roc/{split_name.lower()}_gender\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "# Fairness Audit (gender Ã— emotion)\n",
        "def fairness_audit(df):\n",
        "    print(\"\\nðŸŽ¯ Accuracy by Gender Ã— Emotion:\")\n",
        "    table = (\n",
        "        df.groupby([\"true_gender\", \"true_emotion\"])\n",
        "          .apply(lambda g: (g[\"true_emotion\"] == g[\"pred_emotion\"]).mean())\n",
        "          .unstack().fillna(0)\n",
        "    )\n",
        "    print(table.round(3))\n",
        "\n",
        "    for g in table.index:\n",
        "        for e in table.columns:\n",
        "            wandb.log({f\"fairness/{g}/{e}\": table.loc[g, e]})\n",
        "\n",
        "    pos_rate = df[\"pred_gender\"].value_counts(normalize=True)\n",
        "    disp_impact = pos_rate.get(\"Woman\", 0) / pos_rate.get(\"Man\", 1e-6)\n",
        "    wandb.log({\"disparate_impact_ratio\": disp_impact})\n",
        "\n",
        "    equal_opp = table.loc[\"Woman\"].mean() - table.loc[\"Man\"].mean()\n",
        "    wandb.log({\"equal_opportunity_diff\": equal_opp})\n",
        "\n",
        "    table.T.plot(kind=\"bar\", figsize=(10, 6), ylim=(0, 1))\n",
        "    plt.title(\"Emotion Accuracy by Gender\"); plt.ylabel(\"Accuracy\")\n",
        "    plt.grid(axis=\"y\"); plt.tight_layout(); plt.show()\n",
        "    wandb.log({\"gender_bias_bar\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "#  Model Initialization\n",
        "model = MultiTaskCNN().to(DEVICE)\n",
        "\n",
        "#  Optimizer, Scheduler, Loss\n",
        "#initial_lr = 1e-3\n",
        "#opt = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-4)\n",
        "\n",
        "# Smarter scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=3, gamma=0.5)\n",
        "\n",
        "initial_lr = 3e-4\n",
        "opt = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-4)\n",
        "\n",
        "# linear warmup first 3 epochs then cosine\n",
        "def lr_lambda(cur_epoch):\n",
        "    warmup = 3\n",
        "    if cur_epoch < warmup:\n",
        "        return (cur_epoch + 1) / warmup\n",
        "    # cosine from warmup to EPOCHS\n",
        "    t = (cur_epoch - warmup) / max(1, (EPOCHS - warmup))\n",
        "    return 0.5 * (1 + np.cos(np.pi * t))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "# Label-smoothed loss for better generalization\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "#  Tracking\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_emo_losses = []\n",
        "train_gen_losses = []\n",
        "val_emo_losses = []\n",
        "val_gen_losses = []\n",
        "\n",
        "train_emo_accuracies = []\n",
        "train_gen_accuracies = []\n",
        "val_emo_accuracies = []\n",
        "val_gen_accuracies = []\n",
        "\n",
        "# Early stopping setup\n",
        "patience = 5\n",
        "min_delta = 1e-3\n",
        "best_val_loss = np.inf\n",
        "best_val_metric = 0\n",
        "best_epoch = 0\n",
        "no_imp = 0\n",
        "\n",
        "#  Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    model.train()\n",
        "    tl = train_loader\n",
        "    total_loss = total_emo_loss = total_gen_loss = 0\n",
        "    correct_emo = correct_gen = total_emo = total_gen = 0\n",
        "\n",
        "    for imgs, emo_y, gen_y in tqdm(tl, desc=\"Train\", leave=False):\n",
        "        imgs, emo_y, gen_y = imgs.to(DEVICE), emo_y.to(DEVICE), gen_y.to(DEVICE)\n",
        "        emo_logit, gen_logit = model(imgs)\n",
        "\n",
        "        loss_e = criterion(emo_logit, emo_y)\n",
        "        loss_g = criterion(gen_logit, gen_y)\n",
        "        loss = 0.7 * loss_e + 0.3 * loss_g\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        bs = imgs.size(0)\n",
        "        total_loss     += loss.item()\n",
        "        total_emo_loss += loss_e.item()\n",
        "        total_gen_loss += loss_g.item()\n",
        "        correct_emo    += (emo_logit.argmax(1) == emo_y).sum().item()\n",
        "        correct_gen    += (gen_logit.argmax(1) == gen_y).sum().item()\n",
        "        total_emo      += bs\n",
        "        total_gen      += bs\n",
        "\n",
        "    train_losses.append(total_loss / len(tl))\n",
        "    train_emo_losses.append(total_emo_loss / len(tl))\n",
        "    train_gen_losses.append(total_gen_loss / len(tl))\n",
        "    train_emo_accuracies.append(correct_emo / total_emo)\n",
        "    train_gen_accuracies.append(correct_gen / total_gen)\n",
        "\n",
        "    #  Validation\n",
        "    model.eval()\n",
        "    vl = val_loader\n",
        "    val_loss_total = val_emo_loss = val_gen_loss = 0\n",
        "    val_correct_emo = val_correct_gen = val_total_emo = val_total_gen = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, emo_y, gen_y in vl:\n",
        "            imgs, emo_y, gen_y = imgs.to(DEVICE), emo_y.to(DEVICE), gen_y.to(DEVICE)\n",
        "            emo_logit, gen_logit = model(imgs)\n",
        "\n",
        "            loss_e = criterion(emo_logit, emo_y)\n",
        "            loss_g = criterion(gen_logit, gen_y)\n",
        "            val_loss_total += (loss_e + loss_g).item()\n",
        "            val_emo_loss   += loss_e.item()\n",
        "            val_gen_loss   += loss_g.item()\n",
        "            val_correct_emo += (emo_logit.argmax(1) == emo_y).sum().item()\n",
        "            val_correct_gen += (gen_logit.argmax(1) == gen_y).sum().item()\n",
        "            val_total_emo  += imgs.size(0)\n",
        "            val_total_gen  += imgs.size(0)\n",
        "\n",
        "    val_losses.append(val_loss_total / len(vl))\n",
        "    val_emo_losses.append(val_emo_loss / len(vl))\n",
        "    val_gen_losses.append(val_gen_loss / len(vl))\n",
        "    val_emo_accuracies.append(val_correct_emo / val_total_emo)\n",
        "    val_gen_accuracies.append(val_correct_gen / val_total_gen)\n",
        "\n",
        "    print(f\" Train loss {train_losses[-1]:.4f}, Val loss {val_losses[-1]:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch+1,\n",
        "        \"train_loss\": train_losses[-1],\n",
        "        \"val_loss\":   val_losses[-1],\n",
        "        \"train_emo_acc\": train_emo_accuracies[-1],\n",
        "        \"val_emo_acc\":   val_emo_accuracies[-1],\n",
        "        \"train_gen_acc\": train_gen_accuracies[-1],\n",
        "        \"val_gen_acc\":   val_gen_accuracies[-1],\n",
        "        \"lr\": opt.param_groups[0]['lr'],\n",
        "    })\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Hybrid Earlyâ€‘stopping (Loss + Accuracy) #\n",
        "    val_metric = (val_emo_accuracies[-1] + val_gen_accuracies[-1]) / 2\n",
        "    loss_improved = val_losses[-1] < best_val_loss - min_delta\n",
        "    acc_improved  = val_metric     > best_val_metric + min_delta\n",
        "\n",
        "    if loss_improved or acc_improved:\n",
        "        best_val_loss = min(best_val_loss, val_losses[-1])\n",
        "        best_val_metric = max(best_val_metric, val_metric)\n",
        "        best_epoch = epoch\n",
        "        no_imp = 0\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(\" Model improved, checkpoint saved.\")\n",
        "    else:\n",
        "        no_imp += 1\n",
        "        print(f\"No improvement for {no_imp} epoch(s).\")\n",
        "        if no_imp >= patience:\n",
        "            print(\"Earlyâ€‘stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n",
        "#  Final Eval\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "train_df = evaluate(model, train_loader, \"train\")\n",
        "val_df   = evaluate(model, val_loader, \"val\")\n",
        "test_df  = evaluate(model, test_loader, \"test\")\n",
        "test_df.to_csv(PRED_CSV, index=False)\n",
        "print(\"Predictions saved.\")\n",
        "\n",
        "plot_confusion(train_df, \"Train\")\n",
        "plot_confusion(val_df, \"Val\")\n",
        "plot_confusion(test_df, \"Test\")\n",
        "\n",
        "print_classification_reports(train_df, \"Train\")\n",
        "print_classification_reports(val_df,   \"Validation\")\n",
        "print_classification_reports(test_df,  \"Test\")\n",
        "\n",
        "\n",
        "plot_accuracy_curves(\n",
        "    train_emo_accuracies, val_emo_accuracies,\n",
        "    train_gen_accuracies, val_gen_accuracies\n",
        ")\n",
        "plot_loss_curves(\n",
        "    train_losses, val_losses,\n",
        "    train_emo_losses, val_emo_losses,\n",
        "    train_gen_losses, val_gen_losses\n",
        ")\n",
        "\n",
        "et_e_t, ep_t, gt_t, gp_t, pro_e_t, pro_g_t = collect_preds(test_loader, return_probs=True)\n",
        "plot_extended_roc(\"Test\", et_e_t, pro_e_t, gt_t, pro_g_t)\n",
        "\n",
        "fairness_audit(test_df)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "rliJTNw0ZpFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EfficientNet_B2 Model Training (Change data paths for each dataset**)"
      ],
      "metadata": {
        "id": "T9xF8mIvbvZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Multitask Emotion & Gender Classification with EfficientNet_B2, Fairness, and wandb ===\n",
        "#\n",
        "\n",
        "import os, cv2, io, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from itertools import cycle\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             roc_curve, auc)\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "import scipy.ndimage as snd\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torchvision.models import efficientnet_b2\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import wandb\n",
        "wandb.init(project=\"multitask-gender-emotion\", name=\"Celeb_A-Scratchâ€‘EfficentNet_B2\")\n",
        "\n",
        "# Config\n",
        "emotion_labels  = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
        "gender_labels   = [\"Man\", \"Woman\"]\n",
        "\n",
        "IMG_SIZE  = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS   = 50\n",
        "LR = 1e-3\n",
        "\n",
        "CSV_PATH      = \"/content/dataset/CK+/processed/CKPLUS_Features_20250805_111308_faceprefixed_CLEAN.csv\"\n",
        "TRAIN_IMG_DIR = \"/content/dataset/CK+/processed/train\"\n",
        "VAL_IMG_DIR   = \"/content/dataset/CK+/processed/val\"\n",
        "TEST_IMG_DIR  = \"/content/dataset/CK+/processed/test\"\n",
        "\n",
        "\n",
        "MODEL_PATH = \"multitask_model_Scratch_efficientnetb2.pt\"\n",
        "PRED_CSV   = \"multitask_predictions_Sratch_efficientnetb2.csv\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "emo_le = LabelEncoder().fit(emotion_labels)\n",
        "gen_le = LabelEncoder().fit(gender_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "#  Optimized Train Transform for CK+ Dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.90, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomRotation(8),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "#  Evaluation Transform (No Augmentation)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Your Dataset Class (unchanged)\n",
        "class AffectNetDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None, verbose=True):\n",
        "        self.df, self.img_dir, self.transform, self.verbose = df.reset_index(drop=True), img_dir, transform, verbose\n",
        "        print(f\"CKplus â†’ {len(self.df)} samples from {self.img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        path = os.path.join(self.img_dir, row[\"filename\"])\n",
        "\n",
        "        if self.verbose and idx % 1000 == 0:\n",
        "            print(f\"[INFO] Loading {idx}/{len(self.df)}: {path}\")\n",
        "\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            img = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        img = self.transform(img) if self.transform else transforms.ToTensor()(img / 255.)\n",
        "\n",
        "        emo = torch.tensor(emo_le.transform([row[\"dominant_emotion\"]])[0])\n",
        "        gen = torch.tensor(gen_le.transform([row[\"gender\"]])[0])\n",
        "        return img, emo, gen\n",
        "\n",
        "#  Your make_loader Function\n",
        "def make_loader(df, img_dir, *, shuffle, train):\n",
        "    tfm = train_transform if train else eval_transform\n",
        "    return DataLoader(\n",
        "        AffectNetDataset(df, img_dir, transform=tfm),\n",
        "        batch_size=BATCH_SIZE, shuffle=shuffle, num_workers=0\n",
        "    )\n",
        "\n",
        "\n",
        "#  New: Create all loaders\n",
        "def create_dataloaders(csv_path=CSV_PATH,\n",
        "                       train_img_dir=TRAIN_IMG_DIR,\n",
        "                       val_img_dir=VAL_IMG_DIR,\n",
        "                       test_img_dir=TEST_IMG_DIR,\n",
        "                       batch_size=BATCH_SIZE):\n",
        "    # Load full CSV\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Split df by your 'split' column (make sure your CSV has this)\n",
        "    train_df = df[df['dataset'] == 'train'].reset_index(drop=True)\n",
        "    val_df = df[df['dataset'] == 'val'].reset_index(drop=True)\n",
        "    test_df = df[df['dataset'] == 'test'].reset_index(drop=True)\n",
        "\n",
        "    # Create loaders using your make_loader func and existing transforms\n",
        "    train_loader = make_loader(train_df, train_img_dir, shuffle=True, train=True)\n",
        "    val_loader = make_loader(val_df, val_img_dir, shuffle=False, train=False)\n",
        "    test_loader = make_loader(test_df, test_img_dir, shuffle=False, train=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def mixup_data(x, y1, y2, alpha=0.4):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y1_a, y1_b = y1, y1[index]\n",
        "    y2_a, y2_b = y2, y2[index]\n",
        "    return mixed_x, y1_a, y1_b, y2_a, y2_b, lam\n",
        "\n",
        "\n",
        "#  Example of usage\n",
        "train_loader, val_loader, test_loader = create_dataloaders()\n",
        "\n",
        "\n",
        "#  Multiâ€‘Task EfficientNetâ€‘B2\n",
        "class MultiTaskCNN(nn.Module):  # same class name for compatibility\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        backbone = efficientnet_b2(weights=None)\n",
        "        self.features = backbone.features\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        in_feats = backbone.classifier[1].in_features  # usually 1408 for B2\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.head_emo = nn.Linear(in_feats, len(emotion_labels))\n",
        "        self.head_gen = nn.Linear(in_feats, len(gender_labels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        return self.head_emo(x), self.head_gen(x)\n",
        "\n",
        "# Evaluation util\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, split_name=\"\"):\n",
        "    model.eval()\n",
        "    true_e, pred_e, true_g, pred_g = [], [], [], []\n",
        "    for imgs, emo_y, gen_y in tqdm(loader, desc=f\"Evaluating {split_name}\", leave=False):\n",
        "        imgs = imgs.to(DEVICE); emo_y = emo_y.to(DEVICE); gen_y = gen_y.to(DEVICE)\n",
        "        emo_logit, gen_logit = model(imgs)\n",
        "        true_e += emo_y.cpu().tolist()\n",
        "        pred_e += emo_logit.argmax(1).cpu().tolist()\n",
        "        true_g += gen_y.cpu().tolist()\n",
        "        pred_g += gen_logit.argmax(1).cpu().tolist()\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"true_emotion\": emo_le.inverse_transform(np.array(true_e)),\n",
        "        \"pred_emotion\": emo_le.inverse_transform(np.array(pred_e)),\n",
        "        \"true_gender\":  gen_le.inverse_transform(np.array(true_g)),\n",
        "        \"pred_gender\":  gen_le.inverse_transform(np.array(pred_g)),\n",
        "    })\n",
        "\n",
        "#  Confâ€‘Matrix helper\n",
        "def plot_confusion(df, title=\"Confusion\"):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    cm_e = confusion_matrix(df[\"true_emotion\"], df[\"pred_emotion\"],\n",
        "                            labels=emotion_labels, normalize='true')*100\n",
        "    sns.heatmap(cm_e, annot=True, fmt=\".1f\", cmap=\"Blues\",\n",
        "                xticklabels=emotion_labels, yticklabels=emotion_labels,\n",
        "                cbar_kws={'label':'%'}, ax=axes[0])\n",
        "    axes[0].set_title(f\"{title} â€“ Emotion\"); axes[0].set_xlabel(\"Pred\"); axes[0].set_ylabel(\"True\")\n",
        "\n",
        "    cm_g = confusion_matrix(df[\"true_gender\"], df[\"pred_gender\"],\n",
        "                            labels=gender_labels, normalize='true')*100\n",
        "    sns.heatmap(cm_g, annot=True, fmt=\".1f\", cmap=\"Greens\",\n",
        "                xticklabels=gender_labels, yticklabels=gender_labels,\n",
        "                cbar_kws={'label':'%'}, ax=axes[1])\n",
        "    axes[1].set_title(f\"{title} â€“ Gender\"); axes[1].set_xlabel(\"Pred\"); axes[1].set_ylabel(\"True\")\n",
        "\n",
        "    plt.tight_layout(); plt.show()\n",
        "    wandb.log({f\"{title}_conf_matrix\": wandb.Image(fig)})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "#  Classification reports, curves, ROC plots\n",
        "#\n",
        "def print_classification_reports(df, split_name=\"\"):\n",
        "    print(f\"\\n {split_name} Emotion Classification Report:\")\n",
        "    print(classification_report(df[\"true_emotion\"],\n",
        "                                df[\"pred_emotion\"],\n",
        "                                target_names=emotion_labels))\n",
        "\n",
        "    print(f\"\\n {split_name} Gender Classification Report:\")\n",
        "    print(classification_report(df[\"true_gender\"],\n",
        "                                df[\"pred_gender\"],\n",
        "                                target_names=gender_labels))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #  internal helper\n",
        "def _smooth(arr, win: int = 2, *, do: bool = True):\n",
        "    \"\"\"Uniform 1â€‘D smoothing for nicer curves.\"\"\"\n",
        "    if not do or len(arr) < win:\n",
        "        return np.asarray(arr)\n",
        "    return snd.uniform_filter1d(np.asarray(arr, dtype=float), size=win, mode=\"nearest\")\n",
        "\n",
        "\n",
        "# Plot Accuracy Curves\n",
        "def plot_accuracy_curves(train_emo_accs, val_emo_accs,\n",
        "                         train_gen_accs,  val_gen_accs,\n",
        "                         *, smooth: bool = True):\n",
        "    epochs = np.arange(1, len(train_emo_accs) + 1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_emo_accs, do=smooth), label=\"TrainÂ EmotionÂ Acc\",  linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_emo_accs,  do=smooth), label=\"ValÂ Â EmotionÂ Acc\",   linewidth=2)\n",
        "    plt.plot(epochs, _smooth(train_gen_accs, do=smooth), \"--\", label=\"TrainÂ GenderÂ Acc\", linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_gen_accs,  do=smooth), \"--\", label=\"ValÂ Â GenderÂ Acc\",  linewidth=2)\n",
        "\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.ylim(0, 1)\n",
        "    plt.grid(True, alpha=.3); plt.legend(); plt.tight_layout(); plt.show()\n",
        "    wandb.log({\"accuracy_curves\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "#  Plot Loss Curves\n",
        "def plot_loss_curves(train_losses, val_losses,\n",
        "                     train_emo,   val_emo,\n",
        "                     train_gen,   val_gen,\n",
        "                     *, smooth: bool = True):\n",
        "    epochs = np.arange(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_losses, do=smooth), label=\"TrainÂ Total\", linewidth=2)\n",
        "    plt.plot(epochs, _smooth(val_losses,   do=smooth), label=\"ValÂ Â Total\",  linewidth=2)\n",
        "\n",
        "    plt.plot(epochs, _smooth(train_emo, do=smooth), \"--\", label=\"TrainÂ Emotion\")\n",
        "    plt.plot(epochs, _smooth(val_emo,   do=smooth), \"--\", label=\"ValÂ Â Emotion\")\n",
        "    plt.plot(epochs, _smooth(train_gen, do=smooth), \":\",  label=\"TrainÂ Gender\")\n",
        "    plt.plot(epochs, _smooth(val_gen,   do=smooth), \":\",  label=\"ValÂ Â Gender\")\n",
        "\n",
        "    plt.title(\"Loss Curves\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True, alpha=.3)\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "    wandb.log({\"loss_curves\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "#  Collect predictions helper (with optional probabilities)\n",
        "@torch.no_grad()\n",
        "def collect_preds(loader, return_probs: bool = False):\n",
        "    model.eval()\n",
        "    emo_true, emo_pred, gen_true, gen_pred = [], [], [], []\n",
        "    emo_probs, gen_probs = [], []\n",
        "\n",
        "    for imgs, e_y, g_y in loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        e_y  = e_y.to(DEVICE)\n",
        "        g_y  = g_y.to(DEVICE)\n",
        "\n",
        "        emo_logits, gen_logits = model(imgs)\n",
        "\n",
        "        emo_true.extend(e_y.cpu().numpy())\n",
        "        gen_true.extend(g_y.cpu().numpy())\n",
        "        emo_pred.extend(torch.argmax(emo_logits, 1).cpu().numpy())\n",
        "        gen_pred.extend(torch.argmax(gen_logits, 1).cpu().numpy())\n",
        "\n",
        "        if return_probs:\n",
        "            emo_probs.append(torch.softmax(emo_logits, 1).cpu())\n",
        "            gen_probs.append(torch.softmax(gen_logits, 1).cpu())\n",
        "\n",
        "    if return_probs:\n",
        "        return (\n",
        "            np.array(emo_true), np.array(emo_pred),\n",
        "            np.array(gen_true), np.array(gen_pred),\n",
        "            torch.cat(emo_probs).numpy(), torch.cat(gen_probs).numpy()\n",
        "        )\n",
        "    else:\n",
        "        return (\n",
        "            np.array(emo_true), np.array(emo_pred),\n",
        "            np.array(gen_true), np.array(gen_pred)\n",
        "        )\n",
        "\n",
        "# ensure gender prob arrays have two columns (binary case)\n",
        "def _ensure_two_cols(arr):\n",
        "    \"\"\"If arr is shape [N] or [N,1], convert to [N,2] as [1-p, p].\"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1 or (arr.ndim == 2 and arr.shape[1] == 1):\n",
        "        arr = arr.reshape(-1, 1)\n",
        "        # Assuming the single column is the probability for the second class (Woman)\n",
        "        # You might need to adjust this based on your LabelEncoder mapping\n",
        "        arr = np.hstack([(1.0 - arr), arr])\n",
        "    return arr\n",
        "\n",
        "\n",
        "#  Extended ROC Curve Plot\n",
        "def plot_extended_roc(split_name, true_emo, prob_emo, true_gen, prob_gen):\n",
        "    true_emo_bin = label_binarize(true_emo, classes=list(range(len(emotion_labels))))\n",
        "    true_gen_bin = label_binarize(true_gen, classes=list(range(len(gender_labels))))\n",
        "\n",
        "    # Emotion ROC\n",
        "    fpr_e, tpr_e, roc_auc_e = {}, {}, {}\n",
        "    for i in range(len(emotion_labels)):\n",
        "        fpr_e[i], tpr_e[i], _ = roc_curve(true_emo_bin[:, i], prob_emo[:, i])\n",
        "        roc_auc_e[i] = auc(fpr_e[i], tpr_e[i])\n",
        "\n",
        "    fpr_e[\"micro\"], tpr_e[\"micro\"], _ = roc_curve(true_emo_bin.ravel(), prob_emo.ravel())\n",
        "    roc_auc_e[\"micro\"] = auc(fpr_e[\"micro\"], tpr_e[\"micro\"])\n",
        "\n",
        "    all_fpr = np.unique(np.concatenate([fpr_e[i] for i in range(len(emotion_labels))]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(len(emotion_labels)):\n",
        "        mean_tpr += np.interp(all_fpr, fpr_e[i], tpr_e[i])\n",
        "    mean_tpr /= len(emotion_labels)\n",
        "    fpr_e[\"macro\"], tpr_e[\"macro\"] = all_fpr, mean_tpr\n",
        "    roc_auc_e[\"macro\"] = auc(fpr_e[\"macro\"], tpr_e[\"macro\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(fpr_e[\"micro\"], tpr_e[\"micro\"],\n",
        "             linestyle='--', label=f\"Microâ€‘avg (AUCÂ =Â {roc_auc_e['micro']:.2f})\")\n",
        "    plt.plot(fpr_e[\"macro\"], tpr_e[\"macro\"],\n",
        "             label=f\"Macroâ€‘avg (AUCÂ =Â {roc_auc_e['macro']:.2f})\")\n",
        "    colors = cycle(sns.color_palette(\"hls\", len(emotion_labels)))\n",
        "    for i, c in zip(range(len(emotion_labels)), colors):\n",
        "        plt.plot(fpr_e[i], tpr_e[i], color=c,\n",
        "                 label=f\"{emotion_labels[i]} (AUCÂ =Â {roc_auc_e[i]:.2f})\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--'); plt.title(f\"{split_name} Emotion ROC\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(fontsize=\"small\"); plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()\n",
        "    wandb.log({f\"roc/{split_name.lower()}_emotion\": wandb.Image(plt)})\n",
        "\n",
        "    # Gender ROC\n",
        "    # Ensure gender probabilities have two columns\n",
        "    prob_gen = _ensure_two_cols(prob_gen)\n",
        "\n",
        "    fpr_g, tpr_g, roc_auc_g = {}, {}, {}\n",
        "    for i in range(len(gender_labels)):\n",
        "        fpr_g[i], tpr_g[i], _ = roc_curve(true_gen_bin[:, i], prob_gen[:, i])\n",
        "        roc_auc_g[i] = auc(fpr_g[i], tpr_g[i])\n",
        "    fpr_g[\"micro\"], tpr_g[\"micro\"], _ = roc_curve(true_gen_bin.ravel(), prob_gen.ravel())\n",
        "    roc_auc_g[\"micro\"] = auc(fpr_g[\"micro\"], tpr_g[\"micro\"])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_g[\"micro\"], tpr_g[\"micro\"],\n",
        "             linestyle='--', label=f\"Microâ€‘avg (AUCÂ =Â {roc_auc_g['micro']:.2f})\")\n",
        "    for i in range(len(gender_labels)):\n",
        "        plt.plot(fpr_g[i], tpr_g[i], label=f\"{gender_labels[i]} (AUCÂ =Â {roc_auc_g[i]:.2f})\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--'); plt.title(f\"{split_name} Gender ROC\")\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.grid(True)\n",
        "    plt.tight_layout(); plt.show()\n",
        "    wandb.log({f\"roc/{split_name.lower()}_gender\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "# Fairness Audit (gender Ã— emotion)\n",
        "def fairness_audit(df):\n",
        "    print(\"\\nðŸŽ¯ Accuracy by Gender Ã— Emotion:\")\n",
        "    table = (\n",
        "        df.groupby([\"true_gender\", \"true_emotion\"])\n",
        "          .apply(lambda g: (g[\"true_emotion\"] == g[\"pred_emotion\"]).mean())\n",
        "          .unstack().fillna(0)\n",
        "    )\n",
        "    print(table.round(3))\n",
        "\n",
        "    for g in table.index:\n",
        "        for e in table.columns:\n",
        "            wandb.log({f\"fairness/{g}/{e}\": table.loc[g, e]})\n",
        "\n",
        "    pos_rate = df[\"pred_gender\"].value_counts(normalize=True)\n",
        "    disp_impact = pos_rate.get(\"Woman\", 0) / pos_rate.get(\"Man\", 1e-6)\n",
        "    wandb.log({\"disparate_impact_ratio\": disp_impact})\n",
        "\n",
        "    equal_opp = table.loc[\"Woman\"].mean() - table.loc[\"Man\"].mean()\n",
        "    wandb.log({\"equal_opportunity_diff\": equal_opp})\n",
        "\n",
        "    table.T.plot(kind=\"bar\", figsize=(10, 6), ylim=(0, 1))\n",
        "    plt.title(\"Emotion Accuracy by Gender\"); plt.ylabel(\"Accuracy\")\n",
        "    plt.grid(axis=\"y\"); plt.tight_layout(); plt.show()\n",
        "    wandb.log({\"gender_bias_bar\": wandb.Image(plt)})\n",
        "\n",
        "\n",
        "#  Model Initialization\n",
        "model = MultiTaskCNN().to(DEVICE)\n",
        "\n",
        "#  Optimizer, Scheduler, Loss\n",
        "#initial_lr = 1e-3\n",
        "#opt = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-4)\n",
        "\n",
        "# Smarter scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=3, gamma=0.5)\n",
        "\n",
        "initial_lr = 3e-4\n",
        "opt = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-4)\n",
        "\n",
        "# linear warmup first 3 epochs then cosine\n",
        "def lr_lambda(cur_epoch):\n",
        "    warmup = 3\n",
        "    if cur_epoch < warmup:\n",
        "        return (cur_epoch + 1) / warmup\n",
        "    # cosine from warmup to EPOCHS\n",
        "    t = (cur_epoch - warmup) / max(1, (EPOCHS - warmup))\n",
        "    return 0.5 * (1 + np.cos(np.pi * t))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "# Label-smoothed loss for better generalization\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "#  Tracking\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_emo_losses = []\n",
        "train_gen_losses = []\n",
        "val_emo_losses = []\n",
        "val_gen_losses = []\n",
        "\n",
        "train_emo_accuracies = []\n",
        "train_gen_accuracies = []\n",
        "val_emo_accuracies = []\n",
        "val_gen_accuracies = []\n",
        "\n",
        "# Early stopping setup\n",
        "patience = 5\n",
        "min_delta = 1e-3\n",
        "best_val_loss = np.inf\n",
        "best_val_metric = 0\n",
        "best_epoch = 0\n",
        "no_imp = 0\n",
        "\n",
        "#  Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    model.train()\n",
        "    tl = train_loader\n",
        "    total_loss = total_emo_loss = total_gen_loss = 0\n",
        "    correct_emo = correct_gen = total_emo = total_gen = 0\n",
        "\n",
        "    for imgs, emo_y, gen_y in tqdm(tl, desc=\"Train\", leave=False):\n",
        "        imgs, emo_y, gen_y = imgs.to(DEVICE), emo_y.to(DEVICE), gen_y.to(DEVICE)\n",
        "        emo_logit, gen_logit = model(imgs)\n",
        "\n",
        "        loss_e = criterion(emo_logit, emo_y)\n",
        "        loss_g = criterion(gen_logit, gen_y)\n",
        "        loss = 0.7 * loss_e + 0.3 * loss_g\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        bs = imgs.size(0)\n",
        "        total_loss     += loss.item()\n",
        "        total_emo_loss += loss_e.item()\n",
        "        total_gen_loss += loss_g.item()\n",
        "        correct_emo    += (emo_logit.argmax(1) == emo_y).sum().item()\n",
        "        correct_gen    += (gen_logit.argmax(1) == gen_y).sum().item()\n",
        "        total_emo      += bs\n",
        "        total_gen      += bs\n",
        "\n",
        "    train_losses.append(total_loss / len(tl))\n",
        "    train_emo_losses.append(total_emo_loss / len(tl))\n",
        "    train_gen_losses.append(total_gen_loss / len(tl))\n",
        "    train_emo_accuracies.append(correct_emo / total_emo)\n",
        "    train_gen_accuracies.append(correct_gen / total_gen)\n",
        "\n",
        "    #  Validation\n",
        "    model.eval()\n",
        "    vl = val_loader\n",
        "    val_loss_total = val_emo_loss = val_gen_loss = 0\n",
        "    val_correct_emo = val_correct_gen = val_total_emo = val_total_gen = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, emo_y, gen_y in vl:\n",
        "            imgs, emo_y, gen_y = imgs.to(DEVICE), emo_y.to(DEVICE), gen_y.to(DEVICE)\n",
        "            emo_logit, gen_logit = model(imgs)\n",
        "\n",
        "            loss_e = criterion(emo_logit, emo_y)\n",
        "            loss_g = criterion(gen_logit, gen_y)\n",
        "            val_loss_total += (loss_e + loss_g).item()\n",
        "            val_emo_loss   += loss_e.item()\n",
        "            val_gen_loss   += loss_g.item()\n",
        "            val_correct_emo += (emo_logit.argmax(1) == emo_y).sum().item()\n",
        "            val_correct_gen += (gen_logit.argmax(1) == gen_y).sum().item()\n",
        "            val_total_emo  += imgs.size(0)\n",
        "            val_total_gen  += imgs.size(0)\n",
        "\n",
        "    val_losses.append(val_loss_total / len(vl))\n",
        "    val_emo_losses.append(val_emo_loss / len(vl))\n",
        "    val_gen_losses.append(val_gen_loss / len(vl))\n",
        "    val_emo_accuracies.append(val_correct_emo / val_total_emo)\n",
        "    val_gen_accuracies.append(val_correct_gen / val_total_gen)\n",
        "\n",
        "    print(f\" Train loss {train_losses[-1]:.4f}, Val loss {val_losses[-1]:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch+1,\n",
        "        \"train_loss\": train_losses[-1],\n",
        "        \"val_loss\":   val_losses[-1],\n",
        "        \"train_emo_acc\": train_emo_accuracies[-1],\n",
        "        \"val_emo_acc\":   val_emo_accuracies[-1],\n",
        "        \"train_gen_acc\": train_gen_accuracies[-1],\n",
        "        \"val_gen_acc\":   val_gen_accuracies[-1],\n",
        "        \"lr\": opt.param_groups[0]['lr'],\n",
        "    })\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Hybrid Earlyâ€‘stopping (Loss + Accuracy) #\n",
        "    val_metric = (val_emo_accuracies[-1] + val_gen_accuracies[-1]) / 2\n",
        "    loss_improved = val_losses[-1] < best_val_loss - min_delta\n",
        "    acc_improved  = val_metric     > best_val_metric + min_delta\n",
        "\n",
        "    if loss_improved or acc_improved:\n",
        "        best_val_loss = min(best_val_loss, val_losses[-1])\n",
        "        best_val_metric = max(best_val_metric, val_metric)\n",
        "        best_epoch = epoch\n",
        "        no_imp = 0\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(\" Model improved, checkpoint saved.\")\n",
        "    else:\n",
        "        no_imp += 1\n",
        "        print(f\"No improvement for {no_imp} epoch(s).\")\n",
        "        if no_imp >= patience:\n",
        "            print(\"Earlyâ€‘stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n",
        "#  Final Eval\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "train_df = evaluate(model, train_loader, \"train\")\n",
        "val_df   = evaluate(model, val_loader, \"val\")\n",
        "test_df  = evaluate(model, test_loader, \"test\")\n",
        "test_df.to_csv(PRED_CSV, index=False)\n",
        "print(\"Predictions saved.\")\n",
        "\n",
        "plot_confusion(train_df, \"Train\")\n",
        "plot_confusion(val_df, \"Val\")\n",
        "plot_confusion(test_df, \"Test\")\n",
        "\n",
        "print_classification_reports(train_df, \"Train\")\n",
        "print_classification_reports(val_df,   \"Validation\")\n",
        "print_classification_reports(test_df,  \"Test\")\n",
        "\n",
        "\n",
        "plot_accuracy_curves(\n",
        "    train_emo_accuracies, val_emo_accuracies,\n",
        "    train_gen_accuracies, val_gen_accuracies\n",
        ")\n",
        "plot_loss_curves(\n",
        "    train_losses, val_losses,\n",
        "    train_emo_losses, val_emo_losses,\n",
        "    train_gen_losses, val_gen_losses\n",
        ")\n",
        "\n",
        "et_e_t, ep_t, gt_t, gp_t, pro_e_t, pro_g_t = collect_preds(test_loader, return_probs=True)\n",
        "plot_extended_roc(\"Test\", et_e_t, pro_e_t, gt_t, pro_g_t)\n",
        "\n",
        "fairness_audit(test_df)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "QiIa-8mlb1yx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}