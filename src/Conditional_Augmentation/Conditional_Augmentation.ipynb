{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Connecting to Google Colab Drive**"
      ],
      "metadata": {
        "id": "ftViq0r0RcMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UVQzXJeRNmC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unzipping Dataset folder content**"
      ],
      "metadata": {
        "id": "CQIaTaPKRlQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/CKplus.zip\" -d \"/content/dataset\""
      ],
      "metadata": {
        "id": "MVD7-LvHRzzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing DeepFace Dependencies"
      ],
      "metadata": {
        "id": "U5CXxMIaR072"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uninstalling TF packages for new version\n",
        "!pip uninstall -y deepface tensorflow keras tf-keras retinaface mtcnn keras-nightly keras-preprocessing\n",
        "\n",
        "#  Compatible stack for Colab (Py 3.12)\n",
        "!pip install -q \"tensorflow==2.19.0\" \"tf-keras==2.19.0\"\n",
        "# Keras 3 ships with TF 2.19 — no need to pin keras separately; let TF manage it.\n",
        "\n",
        "# DeepFace version that exists on PyPI and works with TF 2.19 + tf-keras\n",
        "!pip install -q \"deepface==0.0.95\" \"retina-face==0.0.17\" \"mtcnn==0.1.1\""
      ],
      "metadata": {
        "id": "zJkvlT6dSTBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking how many images would be needed for bias mitigation**"
      ],
      "metadata": {
        "id": "eHoIAwA7VH3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#  INPUTS (edit paths)\n",
        "FAIRNESS_CSV   = \"/content/drive/MyDrive/All_Augmented_gender_fairness_multimodel_results.csv\"\n",
        "TRAIN_CSV      = \"/content/dataset/processed/CKPLUS_Features_20250805_111308_faceprefixed_CLEAN.csv\"\n",
        "SELECT_DATASET = \"CKplus\"\n",
        "#SELECT_MODEL   = \"baseline\"\n",
        "#SELECT_MODEL   = \"MobileNet_V3\"\n",
        "SELECT_MODEL   = \"EfficientNet_B2\"\n",
        "\n",
        "#SELECT_MODEL   = \"EfficientNet_B2\"\n",
        "EMOTIONS       = [\"angry\",\"disgust\",\"fear\",\"happy\",\"sad\",\"surprise\",\"neutral\"]\n",
        "\n",
        "# Fairness target\n",
        "TARGET_GAP_PPTS = 2.0   # acceptable gap (percentage points)\n",
        "\n",
        "# Measured slopes (optional): columns = Dataset,Model,Emotion,Gender,images_added,gap_before,gap_after\n",
        "MEASURED_SLOPE_CSV = None  # e.g. \"/content/augment_round_effects.csv\"\n",
        "\n",
        "# Batch size you typically add per round\n",
        "BATCH_ADD = 50\n",
        "\n",
        "# Heuristic knobs\n",
        "MIN_BOOTSTRAP_N        = 120    # if underrepresented bucket has < this, bring it up to this\n",
        "BALANCE_RATIO_TARGET   = 0.9    # aim for N_under >= 0.9 * N_over\n",
        "GAP_COEF               = 35.0   # images per ppt of excess gap (scaled by 1/sqrt(N_under))\n",
        "ROUND_TO               = BATCH_ADD  # round recommendations to your batch size\n",
        "MAX_ADD_PER_BUCKET     = 2000   # soft safety cap per bucket (tune to your budget)\n",
        "\n",
        "#  LOAD DATA\n",
        "fair = pd.read_csv(FAIRNESS_CSV)\n",
        "fair = fair[(fair[\"Dataset\"] == SELECT_DATASET) & (fair[\"Model\"] == SELECT_MODEL)].copy()\n",
        "\n",
        "# If accuracies are in %, convert to proportions\n",
        "if fair[\"Man_Acc\"].max() > 1.5 or fair[\"Woman_Acc\"].max() > 1.5:\n",
        "    fair[\"Man_Acc\"]   = fair[\"Man_Acc\"]   / 100.0\n",
        "    fair[\"Woman_Acc\"] = fair[\"Woman_Acc\"] / 100.0\n",
        "\n",
        "train = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "# Normalize gender strings to Man/Woman\n",
        "def canon_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in {\"man\",\"male\",\"m\"}: return \"Man\"\n",
        "    if s in {\"woman\",\"female\",\"f\"}: return \"Woman\"\n",
        "    if \"man\" in s: return \"Man\"\n",
        "    if \"woman\" in s: return \"Woman\"\n",
        "    return \"Man\"\n",
        "\n",
        "train[\"gender\"] = train[\"gender\"].apply(canon_gender)\n",
        "train[\"dominant_emotion\"] = train[\"dominant_emotion\"].str.lower()\n",
        "\n",
        "# Count current training images per bucket (gender×emotion) — use 'split' to select training rows\n",
        "counts = (train[train[\"dataset\"].astype(str).str.lower() == \"train\"]\n",
        "          .groupby([\"gender\",\"dominant_emotion\"])[\"filename\"]\n",
        "          .count()\n",
        "          .rename(\"train_count\")\n",
        "          .reset_index())\n",
        "\n",
        "def current_N(gender, emotion):\n",
        "    row = counts[(counts[\"gender\"]==gender) & (counts[\"dominant_emotion\"]==emotion)]\n",
        "    return int(row[\"train_count\"].iloc[0]) if not row.empty else 0\n",
        "\n",
        "#  USE MEASURED SLOPE IF AVAILABLE, ELSE HEURISTIC\n",
        "if MEASURED_SLOPE_CSV:\n",
        "    slopes = pd.read_csv(MEASURED_SLOPE_CSV)\n",
        "    slopes = slopes[(slopes[\"Dataset\"]==SELECT_DATASET) & (slopes[\"Model\"]==SELECT_MODEL)].copy()\n",
        "    slopes[\"gap_before\"] = slopes[\"gap_before\"].astype(float)\n",
        "    slopes[\"gap_after\"]  = slopes[\"gap_after\"].astype(float)\n",
        "    # normalize to 'per 50 images' ppts gain\n",
        "    slopes[\"gain_per_50_ppts\"] = (slopes[\"gap_before\"] - slopes[\"gap_after\"]) * 100.0 \\\n",
        "                                 * (BATCH_ADD / slopes[\"images_added\"].replace(0, np.nan))\n",
        "else:\n",
        "    slopes = None\n",
        "\n",
        "def round_up_to(x, base):\n",
        "    return int(math.ceil(float(x) / base) * base)\n",
        "\n",
        "def estimate_needed_images(gap_ppts, N_under, N_over, gain_per_50_ppts=None, target_gap_ppts=TARGET_GAP_PPTS):\n",
        "    # If we have a measured slope, use it (bounded by cap)\n",
        "    if gain_per_50_ppts is not None and gain_per_50_ppts > 1e-6:\n",
        "        remaining = max(0.0, gap_ppts - target_gap_ppts)\n",
        "        batches = math.ceil(remaining / gain_per_50_ppts)\n",
        "        return min(batches * BATCH_ADD, MAX_ADD_PER_BUCKET)\n",
        "\n",
        "    # 1) Bootstrap floor\n",
        "    if N_under < MIN_BOOTSTRAP_N:\n",
        "        need = MIN_BOOTSTRAP_N - N_under\n",
        "        return min(round_up_to(need, ROUND_TO), MAX_ADD_PER_BUCKET)\n",
        "\n",
        "    # 2) Balance need (toward ratio target)\n",
        "    balance_target = BALANCE_RATIO_TARGET * max(N_over, 1)\n",
        "    balance_need = max(0, int(math.ceil(balance_target - N_under)))\n",
        "\n",
        "    # 3) Gap-driven need (diminishing with sqrt(N_under))\n",
        "    gap_excess = max(0.0, gap_ppts - target_gap_ppts)\n",
        "    if gap_excess <= 0:\n",
        "        gap_need = 0\n",
        "    else:\n",
        "        scale = 1.0 / max(1.0, math.sqrt(N_under))  # more when bucket is small\n",
        "        gap_need = int(math.ceil(GAP_COEF * gap_excess * scale))\n",
        "\n",
        "    # Combine: take the larger of balance or gap need\n",
        "    need = max(balance_need, gap_need)\n",
        "    if need <= 0:\n",
        "        return 0\n",
        "\n",
        "    return min(round_up_to(need, ROUND_TO), MAX_ADD_PER_BUCKET)\n",
        "\n",
        "#  BUILD RECOMMENDATION TABLE\n",
        "rows = []\n",
        "for emo in EMOTIONS:\n",
        "    r = fair[fair[\"Emotion\"] == emo]\n",
        "    if r.empty:\n",
        "        continue\n",
        "    a_m = float(r[\"Man_Acc\"].values[0])\n",
        "    a_w = float(r[\"Woman_Acc\"].values[0])\n",
        "\n",
        "    # handle NaNs safely\n",
        "    if np.isnan(a_m) or np.isnan(a_w):\n",
        "        gap = float(\"nan\")\n",
        "        under_gender = \"Woman\"  # arbitrary; we’ll recommend 0 in NaN case below\n",
        "    else:\n",
        "        gap = abs(a_m - a_w) * 100.0  # ppts\n",
        "        under_gender = \"Man\" if a_m < a_w else \"Woman\"\n",
        "\n",
        "    over_gender  = \"Woman\" if under_gender == \"Man\" else \"Man\"\n",
        "    n_under = current_N(under_gender, emo)\n",
        "    n_over  = current_N(over_gender,  emo)\n",
        "\n",
        "    gain_p50 = None\n",
        "    if slopes is not None:\n",
        "        r2 = slopes[(slopes[\"Emotion\"].str.lower()==emo) & (slopes[\"Gender\"]==under_gender)]\n",
        "        if not r2.empty and np.isfinite(r2[\"gain_per_50_ppts\"].mean()):\n",
        "            gain_p50 = float(r2[\"gain_per_50_ppts\"].mean())\n",
        "\n",
        "    if np.isnan(gap):\n",
        "        rec = 0  # not enough eval data to compute gap; keep 0 or choose a small bootstrap if you prefer\n",
        "        gap_out = float(\"nan\")\n",
        "    else:\n",
        "        rec = estimate_needed_images(\n",
        "            gap_ppts=gap, N_under=n_under, N_over=n_over,\n",
        "            gain_per_50_ppts=gain_p50, target_gap_ppts=TARGET_GAP_PPTS\n",
        "        )\n",
        "        gap_out = round(gap, 2)\n",
        "\n",
        "    rows.append({\n",
        "        \"Emotion\": emo,\n",
        "        \"Gap_ppts\": gap_out,\n",
        "        \"Under_gender\": under_gender,\n",
        "        \"Train_N_under\": n_under,\n",
        "        \"Train_N_over\":  n_over,\n",
        "        \"Gain_per_50_ppts(measured)\": (None if gain_p50 is None else round(gain_p50, 3)),\n",
        "        \"Recommended_add_images\": int(rec)\n",
        "    })\n",
        "\n",
        "recommend = pd.DataFrame(rows).sort_values([\"Gap_ppts\"], ascending=False, na_position=\"last\").reset_index(drop=True)\n",
        "print(recommend.to_string(index=False))\n",
        "\n",
        "recommend.to_csv(\"bias_reduction_recommendations.csv\", index=False)\n",
        "print(\"\\nSaved → bias_reduction_recommendations.csv\")"
      ],
      "metadata": {
        "id": "bAzZDehsVVBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data_Augumentation**"
      ],
      "metadata": {
        "id": "kEgVTOYYSIJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Conditional Augmentation (One-Shot Plan, Resume-Safe, Fast)\n",
        "# - Uses a fixed per-model augmentation plan you provided\n",
        "# - Generates images with SD v1.5 (fast) or SDXL (toggleable)\n",
        "# - Verifies + crops with DeepFace (MTCNN)\n",
        "# - RESIZES CROPPED FACES TO 224x224 (CKplus requirement)\n",
        "# - Saves resume state to avoid losing progress on disconnect\n",
        "# - Merges into flat train folder + appends to train CSV (fixed schema)\n",
        "#\n",
        "\n",
        "# !pip install -q diffusers==0.27.2 transformers accelerate safetensors tqdm pandas opencv-python deepface==0.0.93 mtcnn\n",
        "\n",
        "import os, shutil, cv2, numpy as np, pandas as pd, torch\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, Tuple, List\n",
        "from diffusers import StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from deepface import DeepFace\n",
        "\n",
        "#\n",
        "# CONFIG DATASET PATHS(CKplus, AffectNet, CelebA, FER2013+)\n",
        "# ----------------\n",
        "TRAIN_IMG_DIR  = \"/content/dataset/processed/train\"   # flat folder of training images\n",
        "TRAIN_CSV      = \"/content/dataset/processed/CKPLUS_Features_20250805_111308_faceprefixed_CLEAN.csv\"\n",
        "\n",
        "SELECT_DATASET = \"CKplus\"            # Dataset Selection : CKplus\n",
        "SELECT_MODEL   = \"baseline\"      # \"baseline\" | \"MobileNet_V3\" | \"EfficientNet_V3\"\n",
        "\n",
        "SAVE_DIR  = \"conditional_aug\"\n",
        "ROUND_TAG = \"round_04\"\n",
        "\n",
        "# Generation speed/quality\n",
        "GENERATOR_MODEL    = \"sd15\"         # \"sd15\" (fast) or \"sdxl\"\n",
        "NUM_STEPS          = 20\n",
        "GUIDANCE_SCALE     = 7.5\n",
        "TORCH_DTYPE        = torch.float16\n",
        "DEVICE             = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "GLOBAL_SEED        = 12345\n",
        "BATCH_N            = 2\n",
        "\n",
        "# DeepFace thresholds & detector\n",
        "GENDER_CONF_THRESHOLD  = 0.90\n",
        "EMOTION_CONF_THRESHOLD = 0.90\n",
        "DETECTOR_BACKEND       = \"mtcnn\"\n",
        "ENFORCE_TARGET_MATCH   = True\n",
        "\n",
        "# Labels\n",
        "EMOTIONS = [\"angry\",\"disgust\",\"fear\",\"happy\",\"sad\",\"surprise\",\"neutral\"]\n",
        "\n",
        "# Training CSV schema\n",
        "SCHEMA_COLS = [\n",
        "    \"filename\",\"dataset\",\"split\",\n",
        "    \"gender_pred\",\"gender\",\"dominant_emotion\",\"gender_emotion_class\",\n",
        "    \"angry\",\"disgust\",\"fear\",\"happy\",\"sad\",\"surprise\",\"neutral\",\n",
        "    \"face_x\",\"face_y\",\"face_w\",\"face_h\",\"face_area\",\"face_aspect_ratio\",\n",
        "    \"brightness\",\"sharpness\",\"valence\",\"arousal\"\n",
        "]\n",
        "\n",
        "# Circumplex (for metadata)\n",
        "CIRCUMPLEX_VA = {\n",
        "    \"angry\":   (-0.7, 0.8), \"disgust\": (-0.6, 0.5), \"fear\": (-0.7, 0.9),\n",
        "    \"happy\":   ( 0.8, 0.6), \"sad\":     (-0.8, 0.3), \"surprise\": (0.4, 0.9),\n",
        "    \"neutral\": ( 0.0, 0.0),\n",
        "}\n",
        "\n",
        "#\n",
        "# ONE-SHOT AUGMENTATION PLAN (for each dataset these counts would change based on the heuristics in the cell above : counts)\n",
        "#\n",
        "AUG_PLAN = {\n",
        "    \"baseline\": [\n",
        "        (\"fear\",     \"Man\",   50),\n",
        "        (\"sad\",      \"Woman\", 50),\n",
        "        (\"angry\",    \"Woman\", 100),\n",
        "        (\"happy\",    \"Woman\", 300),\n",
        "        (\"surprise\", \"Woman\", 50),\n",
        "        (\"neutral\",  \"Woman\", 0),\n",
        "        (\"disgust\",  \"Woman\", 0),\n",
        "    ],\n",
        "    \"MobileNet_V3\": [\n",
        "        (\"fear\",     \"Man\",   50),\n",
        "        (\"happy\",    \"Man\",   50),\n",
        "        (\"sad\",      \"Woman\", 50),\n",
        "        (\"angry\",    \"Woman\", 100),\n",
        "        (\"surprise\", \"Woman\", 50),\n",
        "        (\"neutral\",  \"Woman\", 0),\n",
        "        (\"disgust\",  \"Woman\", 0),\n",
        "    ],\n",
        "    \"EfficientNet_B2\": [\n",
        "        (\"happy\",    \"Man\",   100),\n",
        "        (\"fear\",     \"Man\",   50),\n",
        "        (\"sad\",      \"Woman\", 50),\n",
        "        (\"angry\",    \"Woman\", 100),\n",
        "        (\"surprise\", \"Woman\", 50),\n",
        "        (\"neutral\",  \"Woman\", 0),\n",
        "        (\"disgust\",  \"Woman\", 0),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# helpers\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def pillow_to_bgr(pil_image):\n",
        "    rgb = np.array(pil_image.convert(\"RGB\"))\n",
        "    return cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "def build_prompt(gender: str, emotion: str) -> Tuple[str, str]:\n",
        "    pos = (f\"portrait photo of a {gender.lower()} with a clear {emotion.lower()} facial expression, \"\n",
        "           f\"studio lighting, professional photography, sharp focus, detailed skin texture\")\n",
        "    neg = \"blurry, distorted, deformed, cartoon, low-resolution, extra limbs, bad anatomy, jpeg artifacts\"\n",
        "    return pos, neg\n",
        "\n",
        "def brightness_hsv_v(bgr: np.ndarray) -> float:\n",
        "    hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
        "    return float(hsv[..., 2].mean())\n",
        "\n",
        "def sharpness_laplacian(bgr: np.ndarray) -> float:\n",
        "    return float(cv2.Laplacian(bgr, cv2.CV_64F).var())\n",
        "\n",
        "def compute_valence_arousal(emo_probs: Dict[str,float]) -> Tuple[float, float]:\n",
        "    v_sum = a_sum = w_sum = 0.0\n",
        "    for k, p in emo_probs.items():\n",
        "        k = k.lower()\n",
        "        if k in CIRCUMPLEX_VA:\n",
        "            v, a = CIRCUMPLEX_VA[k]\n",
        "            v_sum += v * float(p)\n",
        "            a_sum += a * float(p)\n",
        "            w_sum += float(p)\n",
        "    return (v_sum / w_sum, a_sum / w_sum) if w_sum > 0 else (0.0, 0.0)\n",
        "\n",
        "def _unique_name(dst_dir: str, base: str) -> str:\n",
        "    stem, ext = os.path.splitext(base)\n",
        "    cand = base; k = 1\n",
        "    while os.path.exists(os.path.join(dst_dir, cand)):\n",
        "        cand = f\"{stem}_r{k:04d}{ext}\"\n",
        "        k += 1\n",
        "    return cand\n",
        "\n",
        "def analyze_and_crop_bgr(bgr_img: np.ndarray):\n",
        "    try:\n",
        "        res_all = DeepFace.analyze(\n",
        "            img_path=bgr_img,\n",
        "            actions=[\"emotion\",\"gender\"],\n",
        "            detector_backend=DETECTOR_BACKEND,\n",
        "            enforce_detection=True\n",
        "        )\n",
        "        res = res_all[0] if isinstance(res_all, list) else res_all\n",
        "\n",
        "        dom_emotion = str(res[\"dominant_emotion\"]).lower()\n",
        "        emo_probs   = {str(k).lower(): float(v) for k, v in res[\"emotion\"].items()}\n",
        "        emo_conf    = float(emo_probs.get(dom_emotion, 0.0))\n",
        "\n",
        "        dom_gender  = str(res[\"dominant_gender\"])\n",
        "        gen_probs   = {str(k): float(v) for k, v in res[\"gender\"].items()}\n",
        "        gen_conf    = float(gen_probs.get(dom_gender, 0.0))\n",
        "\n",
        "        region = res.get(\"region\", {})\n",
        "        x, y, w, h = int(region.get(\"x\", 0)), int(region.get(\"y\", 0)), int(region.get(\"w\", 0)), int(region.get(\"h\", 0))\n",
        "        if w <= 0 or h <= 0:\n",
        "            return False, None, {\"reason\": \"empty_crop\"}\n",
        "\n",
        "        H, W = bgr_img.shape[:2]\n",
        "        x, y = max(0, x), max(0, y)\n",
        "        x2, y2 = min(W, x + w), min(H, y + h)\n",
        "        face = bgr_img[y:y2, x:x2]\n",
        "        if face.size == 0:\n",
        "            return False, None, {\"reason\": \"empty_crop\"}\n",
        "\n",
        "        meta = {\n",
        "            \"dominant_emotion\": dom_emotion,\n",
        "            \"emotion_probs\": emo_probs,\n",
        "            \"emotion_conf\": emo_conf,\n",
        "            \"dominant_gender\": dom_gender,\n",
        "            \"gender_probs\": gen_probs,\n",
        "            \"gender_conf\": gen_conf,\n",
        "            \"region\": (x, y, w, h)\n",
        "        }\n",
        "        return True, face, meta\n",
        "    except Exception as e:\n",
        "        return False, None, {\"reason\": f\"deepface_error: {type(e).__name__}: {e}\"}\n",
        "\n",
        "def passes_thresholds(meta: Dict) -> bool:\n",
        "    return (meta.get(\"gender_conf\", 0.0)  >= GENDER_CONF_THRESHOLD) and \\\n",
        "           (meta.get(\"emotion_conf\", 0.0) >= EMOTION_CONF_THRESHOLD)\n",
        "\n",
        "def matches_target(meta: Dict, target_gender: str, target_emotion: str) -> bool:\n",
        "    if not ENFORCE_TARGET_MATCH:\n",
        "        return True\n",
        "    return (str(meta.get(\"dominant_gender\",\"\")).lower() == target_gender.lower()) and \\\n",
        "           (str(meta.get(\"dominant_emotion\",\"\")).lower() == target_emotion.lower())\n",
        "\n",
        "#\n",
        "# LOAD GENERATOR\n",
        "#\n",
        "print(\"Loading generator...\")\n",
        "if GENERATOR_MODEL.lower() == \"sd15\":\n",
        "    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE).to(DEVICE)\n",
        "else:\n",
        "    MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(MODEL_ID, torch_dtype=TORCH_DTYPE, variant=\"fp16\").to(DEVICE)\n",
        "\n",
        "pipe.enable_attention_slicing()\n",
        "try:\n",
        "    pipe.enable_xformers_memory_efficient_attention()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "#\n",
        "# RESUME STATE\n",
        "#\n",
        "round_root = os.path.join(SAVE_DIR, ROUND_TAG, SELECT_DATASET, SELECT_MODEL)\n",
        "ensure_dir(round_root)\n",
        "\n",
        "resume_csv = os.path.join(round_root, \"resume_manifest.csv\")\n",
        "done = set(pd.read_csv(resume_csv)[\"src_key\"].astype(str).tolist()) if os.path.exists(resume_csv) else set()\n",
        "print(f\"Resume: {len(done)} items already saved\")\n",
        "\n",
        "def append_resume(src_key: str):\n",
        "    row = pd.DataFrame([{\"src_key\": src_key}])\n",
        "    row.to_csv(resume_csv, mode=\"a\", header=not os.path.exists(resume_csv), index=False)\n",
        "\n",
        "#\n",
        "# PLAN → FLAG LIST\n",
        "#\n",
        "if SELECT_MODEL not in AUG_PLAN:\n",
        "    raise ValueError(f\"SELECT_MODEL '{SELECT_MODEL}' not in AUG_PLAN keys {list(AUG_PLAN.keys())}\")\n",
        "\n",
        "flag_rows: List[Tuple[str, str, int]] = [(emo, g, int(n)) for (emo, g, n) in AUG_PLAN[SELECT_MODEL] if int(n) > 0]\n",
        "if not flag_rows:\n",
        "    print(\"No augmentation requested by plan. Exiting.\")\n",
        "\n",
        "print(\"Augmentation plan:\", flag_rows)\n",
        "\n",
        "#\n",
        "# GENERATE → VERIFY/CROP(224×224) → SAVE\n",
        "#\n",
        "manifest_rows = []\n",
        "\n",
        "def save_row_and_image(face_bgr, out_dir, fname, meta, emotion, gender):\n",
        "    ensure_dir(out_dir)\n",
        "\n",
        "    #  FORCE 224×224 FOR CKplus\n",
        "    face_bgr = cv2.resize(face_bgr, (224, 224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    fpath = os.path.join(out_dir, fname)\n",
        "    cv2.imwrite(fpath, face_bgr)\n",
        "\n",
        "    fx, fy, fw, fh = meta[\"region\"]\n",
        "    face_area   = float(fw * fh)\n",
        "    face_aspect = round(fw / fh, 4) if fh > 0 else np.nan\n",
        "\n",
        "    bright = round(brightness_hsv_v(face_bgr), 4)\n",
        "    sharp  = round(sharpness_laplacian(face_bgr), 4)\n",
        "\n",
        "    probs = {k: float(meta[\"emotion_probs\"].get(k, 0.0)) for k in EMOTIONS}\n",
        "    v, a = compute_valence_arousal(probs)\n",
        "\n",
        "    manifest_rows.append({\n",
        "        \"filename\": fname,\n",
        "        \"dataset\": SELECT_DATASET,\n",
        "        \"split\": \"train\",\n",
        "        \"gender_pred\": meta[\"dominant_gender\"],\n",
        "        \"gender\": gender,\n",
        "        \"dominant_emotion\": emotion,\n",
        "        \"gender_emotion_class\": f\"{emotion}_{gender}\",\n",
        "        \"angry\": probs[\"angry\"], \"disgust\": probs[\"disgust\"], \"fear\": probs[\"fear\"],\n",
        "        \"happy\": probs[\"happy\"], \"sad\": probs[\"sad\"], \"surprise\": probs[\"surprise\"], \"neutral\": probs[\"neutral\"],\n",
        "        \"face_x\": int(fx), \"face_y\": int(fy), \"face_w\": int(fw), \"face_h\": int(fh),\n",
        "        \"face_area\": face_area, \"face_aspect_ratio\": face_aspect,\n",
        "        \"brightness\": bright, \"sharpness\": sharp,\n",
        "        \"valence\": round(v,4), \"arousal\": round(a,4),\n",
        "    })\n",
        "\n",
        "for (emotion, under_gender, n_target) in flag_rows:\n",
        "    class_dir = os.path.join(round_root, emotion, under_gender)\n",
        "    ensure_dir(class_dir)\n",
        "\n",
        "    pos, neg = build_prompt(under_gender, emotion)\n",
        "\n",
        "    saved = 0\n",
        "    tries = 0\n",
        "    HARD_TRY_CAP = n_target * 6\n",
        "\n",
        "    pbar = tqdm(total=n_target, desc=f\"{under_gender}-{emotion}\")\n",
        "    while saved < n_target and tries < HARD_TRY_CAP:\n",
        "        this_batch = min(BATCH_N, n_target - saved)\n",
        "        gen = torch.Generator(device=DEVICE).manual_seed(GLOBAL_SEED + tries)\n",
        "\n",
        "        if GENERATOR_MODEL.lower() == \"sd15\":\n",
        "            out = pipe(prompt=[pos]*this_batch, negative_prompt=[neg]*this_batch,\n",
        "                       guidance_scale=GUIDANCE_SCALE, num_inference_steps=NUM_STEPS,\n",
        "                       generator=gen)\n",
        "            pil_list = out.images\n",
        "        else:\n",
        "            out = pipe(prompt=[pos]*this_batch, negative_prompt=[neg]*this_batch,\n",
        "                       guidance_scale=GUIDANCE_SCALE, generator=gen)\n",
        "            pil_list = out.images\n",
        "\n",
        "        for pil_img in pil_list:\n",
        "            tries += 1\n",
        "            src_key = f\"{emotion}_{under_gender}_{tries}\"\n",
        "            if src_key in done:\n",
        "                continue\n",
        "\n",
        "            bgr = pillow_to_bgr(pil_img)\n",
        "            ok, face_bgr, meta = analyze_and_crop_bgr(bgr)\n",
        "            if not ok:                      append_resume(src_key); continue\n",
        "            if not passes_thresholds(meta): append_resume(src_key); continue\n",
        "            if not matches_target(meta, under_gender, emotion): append_resume(src_key); continue\n",
        "\n",
        "            fname = f\"{emotion}_{under_gender}_{saved}.png\"\n",
        "            save_row_and_image(face_bgr, class_dir, fname, meta, emotion, under_gender)\n",
        "            append_resume(src_key)\n",
        "\n",
        "            saved += 1\n",
        "            pbar.update(1)\n",
        "            if saved >= n_target:\n",
        "                break\n",
        "\n",
        "    pbar.close()\n",
        "    if saved < n_target:\n",
        "        print(f\" {under_gender}-{emotion}: verified {saved}/{n_target} within try cap ({HARD_TRY_CAP}).\")\n",
        "\n",
        "# Save a manifest (pre-merge)\n",
        "if manifest_rows:\n",
        "    pre_manifest = os.path.join(round_root, f\"manifest_{SELECT_DATASET}_{SELECT_MODEL}_verified.csv\")\n",
        "    pd.DataFrame(manifest_rows).to_csv(pre_manifest, index=False)\n",
        "    print(f\"Pre-merge manifest written: {pre_manifest}\")\n",
        "else:\n",
        "    print(\"No verified crops produced. Exiting early.\")\n",
        "\n",
        "#\n",
        "# MERGE INTO TRAIN\n",
        "#\n",
        "def _unique_name(dst_dir: str, base: str) -> str:\n",
        "    stem, ext = os.path.splitext(base)\n",
        "    cand = base; k = 1\n",
        "    while os.path.exists(os.path.join(dst_dir, cand)):\n",
        "        cand = f\"{stem}_r{k:04d}{ext}\"\n",
        "        k += 1\n",
        "    return cand\n",
        "\n",
        "def merge_manifest_into_train(\n",
        "    manifest_df: pd.DataFrame,\n",
        "    train_csv_path: str,\n",
        "    train_img_dir: str,\n",
        "    gen_root: str,\n",
        "    dataset: str\n",
        "):\n",
        "    if manifest_df is None or manifest_df.empty:\n",
        "        print(\"No rows to merge into train.\"); return\n",
        "\n",
        "    if os.path.exists(train_csv_path):\n",
        "        train_df = pd.read_csv(train_csv_path)\n",
        "    else:\n",
        "        train_df = pd.DataFrame(columns=SCHEMA_COLS)\n",
        "\n",
        "    for col in SCHEMA_COLS:\n",
        "        if col not in train_df.columns:\n",
        "            default = \"train\" if col == \"split\" else (dataset if col == \"dataset\" else np.nan)\n",
        "            train_df[col] = default\n",
        "\n",
        "    appended_rows, copied = [], 0\n",
        "\n",
        "    for _, r in manifest_df.iterrows():\n",
        "        src = os.path.join(gen_root, r[\"dominant_emotion\"], r[\"gender\"], r[\"filename\"])\n",
        "        if not os.path.exists(src):\n",
        "            continue\n",
        "\n",
        "        new_name = _unique_name(train_img_dir, r[\"filename\"])\n",
        "        dst = os.path.join(train_img_dir, new_name)\n",
        "        shutil.copy2(src, dst)\n",
        "        copied += 1\n",
        "\n",
        "        row = r.copy()\n",
        "        row[\"filename\"] = new_name\n",
        "        row[\"split\"]    = \"train\"\n",
        "        row[\"dataset\"]  = dataset\n",
        "        appended_rows.append(row)\n",
        "\n",
        "    if appended_rows:\n",
        "        add_df = pd.DataFrame(appended_rows).reindex(columns=SCHEMA_COLS)\n",
        "        merged = pd.concat([train_df, add_df], ignore_index=True)[SCHEMA_COLS]\n",
        "        merged.to_csv(train_csv_path, index=False)\n",
        "        print(f\"Copied {copied} images → {train_img_dir}\")\n",
        "        print(f\"Appended {len(add_df)} rows to CSV → {train_csv_path}\")\n",
        "    else:\n",
        "        print(\"No files copied; nothing appended to train CSV.\")\n",
        "\n",
        "pre_manifest_path = os.path.join(round_root, f\"manifest_{SELECT_DATASET}_{SELECT_MODEL}_verified.csv\")\n",
        "if os.path.exists(pre_manifest_path):\n",
        "    manifest_df = pd.read_csv(pre_manifest_path)\n",
        "    merge_manifest_into_train(\n",
        "        manifest_df=manifest_df,\n",
        "        train_csv_path=TRAIN_CSV,\n",
        "        train_img_dir=TRAIN_IMG_DIR,\n",
        "        gen_root=round_root,\n",
        "        dataset=SELECT_DATASET\n",
        "    )\n",
        "\n",
        "# Optional: sanity_check_csv_vs_files(TRAIN_CSV, TRAIN_IMG_DIR)"
      ],
      "metadata": {
        "id": "Rn_Rql7WUgjO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}